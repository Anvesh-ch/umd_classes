// Dir is https://drive.google.com/drive/folders/1u8ZUAkLc8yZBwGgXvfBcAY_oSCyzT_pp
// 
// 

::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{8.3: Apache Hadoop}}$$**
\endgroup

::: columns
:::: {.column width=75%}
\vspace{1cm}

- **Instructor**: Dr. GP Saggese - [](gsaggese@umd.edu)

- **Resources**
  - Ghemawat et al.: _The Google File System_, 2003
  - Dean et al.: _MapReduce: Simplified Data Processing on Large Clusters_, 2004
::::
:::: {.column width=20%}
![](data605/lectures_source/images/lecture_7_1/lec_7_1_slide_2_image_1.png)

::::
:::

* Hadoop Distributed File System

::: columns
:::: {.column width=40%}
- **NameNode**
  - Store file/dir hierarchy
  - Store file metadata (location, size, permissions)

- **DataNodes**
  - Store data blocks
  - Split file into 16-64MB blocks
  - Replicate chunks (2x or 3x)
  - Keep replicas in different racks
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_18_image_1.png)
::::
:::

* Hadoop Distributed File System

::: columns
:::: {.column width=40%}
- **Library for file access**
  - Read:
    - Contact _NameNode_ for _DataNode_ and block pointer
    - Connect to _DataNode_ for data access
  - Write:
    - _NameNode_ creates blocks
    - Assign blocks to multiple _DataNodes_
    - Client sends data to _DataNodes_
    - _DataNodes_ store data

- **Client**
  - API (e.g., Python, Java) to library
  - Mount HDFS on local filesystem
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_18_image_1.png)
::::
:::

* Implementations of MapReduce
- **Google**
  - Not available outside Google

- **Hadoop**
  - Open-source in Java
  - Uses HDFS for storage
  - Hadoop Wiki: Intro, Getting Started, Map/Reduce Overview

- **Amazon Elastic MapReduce (EMR)**
  - Hadoop MapReduce on Amazon EC2
  - Also runs Spark, HBase, Hive,

- **Spark**

- **Dask**

* MapReduce: Hadoop

- Hadoop: open-source MapReduce implementation

- Functionalities
  - Partition input data (HDFS)
  - Input adapters
    - E.g., hBase, MongoDB, Cassandra, Amazon Dynamo
  - Schedule program execution across machines
  - Handle machine failures
  - Manage inter-machine communication
  - Perform _GroupByKey_ step
  - Output adapters
    - E.g., avro, ORC, Parquet
  - Schedule multiple _MapReduce_ jobs
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_42_image_1.png){width=80%}

* Data Flow
- Store input, intermediate, final outputs in HDFS
  - Operations in Hadoop move disk to disk

- Use adapters to read/partition data in chunks

- Scheduler places map tasks near physical storage of input data
  - Store intermediate results on local FS of Map and Reduce workers

- Output often serves as input for another MapReduce task

* Hadoop Distributed File System (HDFS): Overview
- Designed for distributed storage of large datasets
- Built on master-slave architecture
  - NameNode: manages metadata and directory structure
  - DataNodes: store actual data blocks
- Optimized for high throughput rather than low latency
- Stores large files across multiple machines
- Writes are append-only, simplifying consistency
- Fault-tolerant using data replication
  - Default: each block is replicated 3 times
- Ideal for batch processing and big data workloads

* HDFS: Data Storage and Access
- Files are split into fixed-size blocks (default: 128MB)
- Blocks distributed across DataNodes for parallelism
- NameNode maintains block-to-DataNode mapping
- Client reads data directly from DataNodes
- Ensures reliability through block replication
- If a DataNode fails, replicas serve the data
- Data locality: computation is moved to where data resides

* Hadoop MapReduce: Overview
- Programming model for distributed data processing
- Processes data in parallel across a cluster
- Consists of two main functions:
  - Map: filters and sorts input data
  - Reduce: aggregates intermediate outputs
- Suited for batch jobs over large datasets
- Fault-tolerant: tasks are retried upon failure

* MapReduce: Execution Phases
- Input data split into chunks processed by Mappers
- Mapper outputs key-value pairs
- Shuffle and Sort: organizes data by key
  - Intermediate keys are grouped and sent to Reducers
- Reducers aggregate values by key
- Final output written back to HDFS

* Example: Word Count with MapReduce
- Input: text files containing words
- Mapper:
  - Reads lines and emits (word, 1) for each word
- Shuffle and Sort:
  - Groups by word, e.g., (word, [1,1,1])
- Reducer:
  - Sums values: emits (word, total_count)
- Output: word frequencies stored in HDFS

* HDFS vs MapReduce: Complementary Roles
- HDFS: distributed storage system
  - Stores large datasets efficiently
- MapReduce: distributed compute model
  - Processes data stored in HDFS
- Together enable scalable and fault-tolerant data analysis

* Benefits and Limitations
- Benefits:
  - Scalable and fault-tolerant
  - Handles petabytes of data
  - Open-source and cost-effective
- Limitations:
  - High latency, not suitable for real-time
  - Difficult for complex iterative algorithms
  - Superseded in many cases by Spark and other engines
