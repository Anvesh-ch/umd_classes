// Dir is https://drive.google.com/drive/folders/1u8ZUAkLc8yZBwGgXvfBcAY_oSCyzT_pp
// 
// 

::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{8.3: Apache Hadoop}}$$**
\endgroup

\vspace{1cm}

::: columns
:::: {.column width=75%}

- **Instructor**: Dr. GP Saggese - [](gsaggese@umd.edu)

- **References**
  - Ghemawat et al.: _The Google File System_, 2003
  - Dean et al.: _MapReduce: Simplified Data Processing on Large Clusters_, 2004
::::
:::: {.column width=20%}

::::
:::

## Hadoop Distributed File System

::: columns
:::: {.column width=60%}
* Hadoop Distributed File System (HDFS)
- **HDFS** is a **distributed file system**
  - Designed to **store large data sets reliably**
  - Part of the Apache Hadoop ecosystem
  - Inspired by the Google File System (GFS)

1. Optimized for **high-throughput access** to large files
   - Suitable for batch processing
   - Not low-latency access

2. Designed for **fault tolerance and scalability**
   - Ensures fault tolerance through replication
     - Default replication factor is 3
   - Blocks are stored on different nodes and racks
   - Follows a primary-secondary architecture
   - Provides data availability even if some nodes fail
   - Replication strategy improves read performance

::::
:::: {.column width=35%}

![](data605/lectures_source/images/lecture_8_1/lec_8_3_slide_1_image_1.png)

![](data605/lectures_source/images/lecture_8_1/lec_8_3_slide_1_image_2.png)

::::
:::

* HDFS Architecture
::: columns
:::: {.column width=50%}
- **NameNode**
  - Store file/dir hierarchy
  - Store file metadata (block location, size, permissions)

- **DataNodes**
  - Store actual data blocks
  - Split file into 16-64MB blocks
  - Replicate chunks (2x or 3x) across multiple _DataNodes_
  - Keep replicas in different racks
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_18_image_1.png)
::::
:::

* Hadoop Distributed File System
::: columns
:::: {.column width=40%}
- **Library for file access**
  - Read:
    - Contact _NameNode_ for _DataNode_ and block pointer
    - Choose the nearest _DataNode_ for each block
    - Connect to _DataNode_ for data access
    - Reads blocks in parallel to improve performance
    - Data is reassembled by the client in correct order
  - Write:
    - _NameNode_ creates blocks
    - Assign blocks to multiple _DataNodes_
    - Client sends data to _DataNodes_
    - _DataNodes_ store data
    - Blocks are pipelined to other replicas
    - Write is considered successful after all replicas acknowledge

- **Client**
  - API (e.g., Python, Java) to library
  - Mount HDFS on local filesystem
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_18_image_1.png)
::::
:::

* Fault Tolerance and Recovery
- _NameNode_ monitors _DataNode_ heartbeat signals
- On failure, blocks are re-replicated to maintain replication factor
- _NameNode_ itself is a single point of failure
  - Solved with HDFS High Availability
- Data integrity ensured using checksums

* HDFS vs Traditional File Systems
- Best for storing and processing large-scale logs, media, sensor data
- Commonly used in data lakes and ETL pipelines
- Optimized for write-once, read-many access pattern
- Supports very large files and directories
  - Performance degrades with many small files
- Lacks low-latency access, but provides high throughput
  - Not suitable for transactional or low-latency systems

## Hadoop MapReduce

* Implementations of MapReduce
- **Google**
  - Not available outside Google

- **Hadoop**
  - Open-source in Java
  - Uses HDFS for storage
  - Hadoop Wiki: Intro, Getting Started, Map/Reduce Overview

- **Amazon Elastic MapReduce (EMR)**
  - Hadoop MapReduce on Amazon EC2
  - Also runs Spark, HBase, Hive,

- **Spark**

- **Dask**

* MapReduce: Hadoop

- Hadoop: open-source MapReduce implementation

- Functionalities
  - Partition input data (HDFS)
  - Input adapters
    - E.g., hBase, MongoDB, Cassandra, Amazon Dynamo
  - Schedule program execution across machines
  - Handle machine failures
  - Manage inter-machine communication
  - Perform _GroupByKey_ step
  - Output adapters
    - E.g., Avro, ORC, Parquet
  - Schedule multiple _MapReduce_ jobs
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_42_image_1.png){width=80%}

* Data Flow
- Store input, intermediate, final outputs in HDFS
  - Operations in Hadoop move disk to disk

- Use adapters to read/partition data in chunks

- Scheduler places map tasks near physical storage of input data
  - Store intermediate results on local FS of Map and Reduce workers

- Output often serves as input for another MapReduce task

* Hadoop Distributed File System (HDFS): Overview
- Designed for distributed storage of large datasets
- Built on master-slave architecture
  - NameNode: manages metadata and directory structure
  - DataNodes: store actual data blocks
- Optimized for high throughput rather than low latency
- Stores large files across multiple machines
- Writes are append-only, simplifying consistency
- Fault-tolerant using data replication
  - Default: each block is replicated 3 times
- Ideal for batch processing and big data workloads

* HDFS: Data Storage and Access
- Files are split into fixed-size blocks (default: 128MB)
- Blocks distributed across DataNodes for parallelism
- NameNode maintains block-to-DataNode mapping
- Client reads data directly from DataNodes
- Ensures reliability through block replication
- If a DataNode fails, replicas serve the data
- Data locality: computation is moved to where data resides

* Hadoop MapReduce: Overview
- Programming model for distributed data processing
- Processes data in parallel across a cluster
- Consists of two main functions:
  - Map: filters and sorts input data
  - Reduce: aggregates intermediate outputs
- Suited for batch jobs over large datasets
- Fault-tolerant: tasks are retried upon failure

* MapReduce: Execution Phases
- Input data split into chunks processed by Mappers
- Mapper outputs key-value pairs
- Shuffle and Sort: organizes data by key
  - Intermediate keys are grouped and sent to Reducers
- Reducers aggregate values by key
- Final output written back to HDFS

* Example: Word Count with MapReduce
- Input: text files containing words
- Mapper:
  - Reads lines and emits (word, 1) for each word
- Shuffle and Sort:
  - Groups by word, e.g., (word, [1,1,1])
- Reducer:
  - Sums values: emits (word, total_count)
- Output: word frequencies stored in HDFS

* HDFS vs MapReduce: Complementary Roles
- HDFS: distributed storage system
  - Stores large datasets efficiently
- MapReduce: distributed compute model
  - Processes data stored in HDFS
- Together enable scalable and fault-tolerant data analysis

* Benefits and Limitations
- Benefits:
  - Scalable and fault-tolerant
  - Handles petabytes of data
  - Open-source and cost-effective
- Limitations:
  - High latency, not suitable for real-time
  - Difficult for complex iterative algorithms
  - Superseded in many cases by Spark and other engines
