::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**Storing and Computing Big Data
**
**MapReduce Framework
**
**(Apache) Hadoop
**
**Algorithms
**
**MapReduce vs DBs
**
\endgroup

::: columns
:::: {.column width=75%}
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`**

**TAs**:
Krishna Pratardan Taduri, kptaduri@umd.edu
Prahar Kaushikbhai Modi, pmodi08@umd.edu

**v1.1**
::::
:::: {.column width=20%}

::::
:::

* UMD DATA605 - Big Data System
- **Storing and Computing Big Data**
- MapReduce Framework
- (Apache) Hadoop
- Algorithms
- MapReduce vs DBs  

* Resources
- Silbershatz: Chap 10
- Seminal papers
  - Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: The Google File System, 2003
  - Jeffrey Dean and Sanjay Ghemawat: MapReduce: Simplified Data Processing on Large Clusters, 2004

* Big Data: Sources and Applications
:::columns
::::{.column width=60%}
- **Growth of World Wide Web in 1990s and 2000s**
- Storing and querying data much larger than enterprise data
- Extremely valuable data to target advertisements and marketing
- Web server logs, web links
- Social media
- Data from mobile phone apps
- Transaction data
- Data from sensors / Internet of Things
- Metadata from communication data
::::
::::{.column width=40%}
\vspace{1cm}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_4_image_1.png)
\footnotesize
**Volume of data in the world
**
::::
:::

* Big Data: Sources and Applications
:::columns
::::{.column width=60%}
- **Big Data characteristics**
- Volume:
  - Amount of data to store and process is much larger than traditional DBs
  - Too big even for parallel DB systems with 10-100 machines
- Velocity
  - Store data at very high rate, due to rate of arrival
  - Data might be processed in real-time (e.g., streaming systems)
- Variety
  - Not all data is relational
  - E.g., semi-structured, textual, graphical data
- **Solution**: process data with 10,000-100,000 machines
::::
::::{.column width=40%}
\vspace{1cm}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_5_image_1.png)
\footnotesize
**Volume of data in the world
**
::::
:::

* Big Data: Sources and Applications
- **Web server logs**
- Record user interactions with web servers
- Billions of users click on thousands links per day $\to$ TB of data / day
- Decide what information (e.g., posts, news) to present to users to keep them "engaged"
  - E.g., what user has viewed, what other similar users have viewed
- Understand visit patterns to optimize for users to find information
- Determine user preferences and trends to inform business decisions
- Decide what advertisements to show to which users
  - Maximize benefit to the advertiser
  - Websites are paid for click-through or conversion
- **Click-through rate**
- A user clicks on an advertisement to get more information
- It is a measure of success in getting user attention/engagement
- **Conversion rate**
  - When a user actually purchases the advertised product or service

* Big Data: Storing and Computing

- Big data needs 10k-100k machines
- **Two problems**
  - Storing big data
  - Processing big data
- **Need to be solved together and *efficiently***
  - If one phase is slow $\to$ the entire system is slow

* Processing the Web: Example
:::columns
::::{.column width=50%}
- The web has:
  - 20+ billion web pages
  - Total 5M TBs = 5 ZB
  - 1M 5TB hard drives to store the web
  - 100/HDD -> 100M to store the web
  - Not too bad!
- One computer reads 300 MB/sec from disk
  - 5e6 * 1024 * 1024 * 8 / 300 / 3600 / 24 / 365
= 4,433 years to read the web serially from disk
- It takes even more to do something useful with the data!
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_8_image_1.png)
::::
:::

* Big Data: Storage Systems
- How can we store big data?
- **Distributed file systems**
  - E.g., store large files like log files
- **Sharding across multiple DBs**
  - Partition records based on shard key across multiple systems
- **Parallel and distributed DBs**
  - Store data / perform queries across multiple machines
  - Use traditional relational DB interface
- **Key-value stores**
  - Data stored and retrieved based on a key
  - Limitations on semantics, consistency, querying with respect to relational DBs
  - E.g., NoSQL DB, Mongo, Redis

* 1 Distributed File Systems
- **Distributed file system**
  - Files stored across a number of machines, giving a single file-system view to clients
    - E.g., Google File System (GFS)
    - E.g., Hadoop File System (HDFS) based on GFS architecture
    - E.g., AWS S3
  - Files are:
    - Broken into multiple blocks
    - Blocks are partitioned across multiple machines
    - Blocks are often replicated across machines
  - **Goals**:
    - Store data that doesn't fit on one machine
    - Increase performance
    - Increase reliability/availability/fault tolerance

* 2 Sharding Across Multiple DBs
- **Sharding** = process of partitioning records across multiple DBs or machines
- Shard keys
  - Aka partitioning keys / partition attributes
- One or more attributes to partition the data
  - Range partition (e.g., timeseries)
  - Hash partition
- **Pros**
  - Scale beyond a centralized DB to handle more users, storage, processing speed
- **Cons**
  - Replication is often needed to deal with failures
  - Ensuring consistency is challenging
  - Relational DBs are not good at supporting constraints (e.g., foreign key) and transactions on multiple machines

* 3 Parallel and Distributed DBs
- **Parallel and distributed DBs**: store and process data running on multiple machines (aka "cluster")
  - E.g., Mongo
- **Pros**
  - Programmer viewpoint
    - Traditional relational DB interface
    - Looks like a DB running on a single machine
  - Can run on 10s-100s of machines
  - Data is replicated for performance and reliability
    - Failures are "frequent" with 100s of machines
    - A query can be just restarted using a different machine
- **Cons**
  - Run a query incrementally requires a lot of complexity
  - Limit to the scalability

* 4 Key-value Stores
- **Problem**
- Many applications (e.g., web) store a very large number (billions or more) small records (few KBs to few MBs)
- File systems can't store such a large number of files
- RDBMSs don't support constraints and transactions on multiple machines
- **Solution**
- Key-value stores / Document / NoSQL systems
- Records are stored, updated, and retrieved based on a key
- Operations are: **put(key, value)** to store, **get(key)** to retrieve data
- **Pros**
- Partition data across multiple machines easily
- Support replication and consistency (no referential integrity)
- Balance workload and add more machines
- **Cons**
- Features are sacrificed to achieve scalability on large clusters
  - Declarative querying
  - Transactions
  - Retrieval based on non-key attributes

* 4 Parallel Key-value Stores
- **Parallel key-value stores**
- BigTable (Google)
- Apache HBase (open source version of BigTable)
- Dynamo, S3 (AWS)
- Cassandra (Facebook)
- Azure cloud storage (Microsoft)
- Redis
- **Parallel document stores**
- MongoDB cluster
- Couchbase
- **In-memory caching systems**
- Store some relations (or parts of relations) into an in-memory cache
- Replicated or partitioned across multiple machines
- E.g., memcached or Redis

* Big Data: Computing Systems
- **How to process Big Data?**
- **Challenges**
  - How to distribute computation?
  - How can we make it easy to write distributed programs?
    - Distributed / parallel programming is hard
  - How to store data in a distributed system?
  - How to survive failures?
    - One server may stay up 3 years (1,000 days)
    - If you have 1,000 servers, expect to lose 1 / day
    - E.g., 1M machines (Google in 2011) $\to$ 1,000 machines fail every day!
- **MapReduce**
  - Solve these problems for certain kinds of computations
  - An elegant way to work with big data
  - Started as Google's data manipulation model
    - (But it wasn't an entirely new idea)

* Cluster Architecture
- Today, a standard architecture for big data computation has emerged:
  - Cluster of commodity Linux nodes
  - Commodity network (typically Ethernet) to connect them
  - In 2011 it was guesstimated that Google had 1M machines, in 2025 ~10-15M (?)
```graphviz
digraph G {
    rankdir=TB;
    bgcolor="white";
    node [fontname="Helvetica", fontsize=14, shape=box, style=solid, margin="0.2,0.1"];
    edge [fontname="Helvetica", fontsize=12];

    // --- TOP LEVEL ---
    label_backbone [label="2-10 Gbps backbone\nbetween racks", shape=plaintext, fontcolor="#003366", fontsize=16];
    switch_top [label="Switch", style=filled, fillcolor="#6F9BCC", fontcolor=white, width=2, height=0.6];

    // --- RACK 1 (Left) - Enclosed in a dashed box ---
    subgraph cluster_rack1 {
        label = "Rack";
        style = "dashed";
        fontcolor = "black";
        fontsize = 16;
        labelloc = "t";
        bgcolor = "transparent";
        
        label_intra_rack [label="1 Gbps between\nany pair of nodes\nin a rack", shape=plaintext, fontcolor="#003366", fontsize=16];
        switch_rack1 [label="Switch", style=filled, fillcolor="#6F9BCC", fontcolor=white, width=2, height=0.6];
        
        // Node 1 (with "Node" label)
        subgraph cluster_node1 {
            label = "Node"; // The "Node" label
            labelloc = "l";
            style = "solid";
            fontcolor = "black";
            bgcolor = "white";
            
            n1_cpu [label="CPU", style="filled", fillcolor="white"];
            n1_mem [label="Mem", style="filled", fillcolor="white"];
            n1_disk [label="Disk", shape=cylinder, style="filled", fillcolor="white", margin="0.3,0.1"];
            n1_cpu -> n1_mem -> n1_disk [style=invis]; // Stack them
        }

        dots_left [label="...", shape=plaintext, fontsize=24];
        
        // Node 2 (no label)
        subgraph cluster_node2 {
            label = ""; // No label for this box
            style = "solid";
            bgcolor = "white";
            
            n2_cpu [label="CPU", style="filled", fillcolor="white"];
            n2_mem [label="Mem", style="filled", fillcolor="white"];
            n2_disk [label="Disk", shape=cylinder, style="filled", fillcolor="white", margin="0.3,0.1"];
            n2_cpu -> n2_mem -> n2_disk [style=invis]; // Stack them
        }

        label_rack_size [label="Each rack contains 16-64 nodes", shape=plaintext, fontcolor="#003366", fontsize=16];

        // --- RACK 1 - EDGES & RANKS ---
        label_intra_rack -> switch_rack1 [style=invis];
        // Connect switch to a node *inside* the cluster; Graphviz clips edge to cluster box
        switch_rack1 -> n1_cpu; 
        switch_rack1 -> dots_left;
        switch_rack1 -> n2_cpu;
        
        { rank=same; dots_left; } // Align nodes horizontally

        n1_cpu -> label_rack_size [style=invis]; // Use internal nodes for positioning
        dots_left -> label_rack_size [style=invis];
        n2_cpu -> label_rack_size [style=invis];
    }

    // --- OTHER RACKS (Right) - NOT in a dashed box ---
    
    switch_rack2 [label="Switch", style=filled, fillcolor="#6F9BCC", fontcolor=white, width=2, height=0.6];

    // Node 3 (no label)
    subgraph cluster_node3 {
        label = ""; // No label
        style = "solid";
        bgcolor = "white";
        
        n3_cpu [label="CPU", style="filled", fillcolor="white"];
        n3_mem [label="Mem", style="filled", fillcolor="white"];
        n3_disk [label="Disk", shape=cylinder, style="filled", fillcolor="white", margin="0.3,0.1"];
        n3_cpu -> n3_mem -> n3_disk [style=invis];
    }

    dots_right [label="...", shape=plaintext, fontsize=24];

    // Node 4 (no label)
    subgraph cluster_node4 {
        label = ""; // No label
        style = "solid";
        bgcolor = "white";
        
        n4_cpu [label="CPU", style="filled", fillcolor="white"];
        n4_mem [label="Mem", style="filled", fillcolor="white"];
        n4_disk [label="Disk", shape=cylinder, style="filled", fillcolor="white", margin="0.3,0.1"];
        n4_cpu -> n4_mem -> n4_disk [style=invis];
    }

    // --- RACK 2 - EDGES & RANKS ---
    switch_rack2 -> n3_cpu;
    switch_rack2 -> dots_right;
    switch_rack2 -> n4_cpu;
    
    { rank=same; dots_right; cluster_node4 } // Align nodes horizontally

    // --- TOP-LEVEL EDGES & RANKS ---
    label_backbone -> switch_top [style=invis];
    switch_top -> switch_rack1;
    switch_top -> switch_rack2;
    
    { rank=same; switch_rack1; switch_rack2 } // Align rack switches
}
```

* Cluster Architecture
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_17_image_1.png)

* Cluster Architecture: Network Bandwidth
- **Problems**
  - Data is hosted on different machines in a cluster
  - Copying data over a network takes time
- **Solutions**
  - Bring computation close to the data
  - Store files multiple times for reliability/performance
- **MapReduce**
  - Addresses both these problems
  - Storage infrastructure: distributed file system
    - Google GFS, Hadoop HDFS
  - Programming model: MapReduce

* Storage Infrastructure

- **Problem**
  - How to store data *persistently* and *efficiently* when nodes can fail?
- **Typical data usage pattern**
  - Huge files (100s of GB to 1TB)
  - Reads and appends are common operations
  - Data is rarely updated in place
- **Solution**
  - Distributed file system
  - Allow files to be stored across a number of machines
  - Files are:
    - Broken into multiple blocks
    - Partitioned across multiple machines
    - Typically with replication across machines
  - Give a single file-system view to clients

* Distributed File System
- Reliable distributed file system
  - Data kept in "**chunks**" spread across machines
  - Each chunk **replicated** on different machines
  - Seamless recovery from disk or machine failure
- Bring computation directly to the data
  - "chunk servers" also serve as "compute servers"

```mermaid
graph LR
    %% --- Define Color Classes ---
    %% These classes set the background, border, and text color for each block type.
    classDef C0 fill:#fcfc04,stroke:#222,color:#333,font-weight:bold
    classDef C1 fill:#fc9c04,stroke:#222,color:#333,font-weight:bold
    classDef C2 fill:#c0c0c0,stroke:#222,color:#333,font-weight:bold
    classDef C3 fill:#04fc04,stroke:#222,color:#333,font-weight:bold
    classDef C5 fill:#a4d8fc,stroke:#222,color:#333,font-weight:bold
    classDef D0 fill:#a4b48c,stroke:#222,color:#333,font-weight:bold
    classDef D1 fill:#a4dc8c,stroke:#222,color:#333,font-weight:bold

    %% --- Chunk server 1 ---
    %% Assign an ID (SG1) and then the display text (the HTML span)
    subgraph SG1 ["<span style='color:#049c9c; font-weight:bold;'>Chunk server 1</span>"]
        direction TB
        %% Use invisible links (~~~) to position nodes in a 2x2 grid
        S1_C0["C₀"]:::C0 ~~~ S1_C1["C₁"]:::C1
        S1_C5["C₅"]:::C5 ~~~ S1_C2["C₂"]:::C2
    end

    %% --- Chunk server 2 ---
    %% Assign an ID (SG2)
    subgraph SG2 ["<span style='color:#049c9c; font-weight:bold;'>Chunk server 2</span>"]
        direction TB
        S2_D0["D₀"]:::D0 ~~~ S2_C1["C₁"]:::C1
        S2_C5["C₅"]:::C5 ~~~ S2_C3["C₃"]:::C3
    end

    %% --- Chunk server 3 ---
    %% Assign an ID (SG3)
    subgraph SG3 ["<span style='color:#049c9c; font-weight:bold;'>Chunk server 3</span>"]
        direction TB
        S3_C2["C₂"]:::C2 ~~~ S3_C5["C₅"]:::C5
        S3_D0["D₀"]:::D0 ~~~ S3_D1["D₁"]:::D1
    end

    %% --- Ellipsis Node ---
    %% A special node for the "..."
    Dots["..."]
    style Dots fill:none,stroke:none,color:#333,font-size:24px,font-weight:bold

    %% --- Chunk server N ---
    %% Assign an ID (SGN)
    subgraph SGN ["<span style='color:#049c9c; font-weight:bold;'>Chunk server N</span>"]
        direction TB
        SN_C0["C₀"]:::C0 ~~~ SN_C5["C₅"]:::C5
        SN_D0["D₀"]:::D0 ~~~ SN_C2["C₂"]:::C2
    end
    
    %% --- Style Subgraphs ---
    %% This applies the teal, dashed border to all subgraphs.
    %% We now list them by their simple IDs (SG1, SG2, etc.)
    style SG1 fill:none,stroke:#049c9c,stroke-width:3px,stroke-dasharray: 8 4
    style SG2 fill:none,stroke:#049c9c,stroke-width:3px,stroke-dasharray: 8 4
    style SG3 fill:none,stroke:#049c9c,stroke-width:3px,stroke-dasharray: 8 4
    style SGN fill:none,stroke:#049c9c,stroke-width:3px,stroke-dasharray: 8 4

    %% --- Link Diagram Parts ---
    %% Use invisible links to force the horizontal layout of the subgraphs
    S1_C2 ~~~ S2_D0
    S2_C3 ~~~ S3_C2
    S3_D1 ~~~ Dots
    Dots ~~~ SN_C0
```

* Hadoop Distributed File System

- **NameNode**
  - Store file / dir hierarchy
  - Store metadata about files (e.g., where are stored, size, permissions)
- **DataNodes**
  - Store data blocks
  - File is split into contiguous 16-64MB blocks
  - Each chunk is replicated (usually 2x or 3x)
  - Keep replicas in different server racks

* Hadoop Distributed File System

- **Library for file access**
  - Read:
    - Talk to *NameNode* to find *DataNode* and pointer to block
    - Connect directly to *DataNode* to access data
  - Write:
    - *NameNode* creates blocks
    - Assign blocks to several *DataNodes*
    - Client sends data to assigned *DataNodes*
    - *DataNodes* store data
- **Client**
  - API (e.g,. Python, Java) to internal library
  - Mount HDFS on local filesystem

* MapReduce: Overview

- **MapReduce programming model**
  - Inspired by functional programming (e.g., Lisp)
  - Common pattern of parallel programming
  - Basic algorithm
    - Given a large number of records to process
    - The same function **map()** is applied to each record
    - Group the results by key
    - A form of aggregation **reduce()** is applied to the result of **map()**
- **Example**
  - **Goal**: sum the length of all the tuples in a document
    - E.g., **[() (a,) (a, b) (a, b, c)]**
  - **map(function, set of values)**
    - Apply function to each value in the set (e.g., len)**map(len, [(), (a), (a, b), (a, b, c))]) ->s [0, 1, 2, 3]**
  - **reduce(function, set of values)**
    - Combine all the values using a binary function (e.g., add) **reduce(add, [0, 1, 2, 3]) -> 6**

* MapReduce: Overview

- Structure of computation stays the same
  - **Read input**
    - Sequentially or in parallel
  - **Map**
    - Extract / compute something from records in the inputs
  - **Group by key**
    - Sort and shuffle
  - **Reduce**
    - Aggregate, summarize, filter, or transform
  - **Write the result**
- MapReduce framework (e.g., Hadoop, Spark) implements the general algorithm
- User specifies the **map()** and **reduce()** functions to solve the problem

* MapReduce: Word Count
- **Word Count**
  - "Hello world" of MapReduce
  - We have a huge text file (so big you can't keep it in memory)
  - Count the number of times each distinct word appears in the file
- **Sample application**
  - Analyze web server logs to find popular URLs
- **Linux solution**
  - Example file from https://en.wikipedia.org/wiki/Hot_Cross_Buns_(song)

:::columns
::::{.column width=70%}
\begingroup\color{blue}
```
      > more doc.txt
      One a penny, two a penny, hot cross buns.
      > words doc.txt | sort | uniq -c
      a 2
      buns 1
      cross 1
      ...
```
\endgroup
::::
::::{.column width=30%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_25_image_1.png){width=80%}
::::
:::

- **words** takes a file and outputs the words one per line
- This Unix pipeline is naturally parallelizable in a MapReduce sense

* MapReduce: Word Count
:::columns
::::{.column width=33%}
**Action**
\footnotesize
```

Read input
```

\vspace{0.15cm}

```
Map:
```
- Invoke **map**() on each input record
- Emit 0 or more output data items
\vspace{0.5cm}

```
Group by key:
```
- Gather all outputs from **map**() stage
- Collect outputs by keys

\vspace{0.2cm}

```
Reduce:
```
- Combine the list of outputs with same keys
::::
::::{.column width=33%}
**Python code**
\footnotesize
```python

values = read(file_name)

def map(values):
  # values: words in document
  for word in values:
		emit(word, 1)







def reduce(key, values):
   # key: a word
   # value: a list of counts
   result = 0
   # result = sum(values)
   for count in values:
	result += count
   emit(key, result)
```
::::
::::{.column width=33%}
**Example**
\footnotesize
\begingroup\color{blue}\tiny
```

    "One a penny, two a penny, 
    hot cross buns."
```
\endgroup

```
  Map:
```
\begingroup\tiny\color{blue}
```
    [("one", 1), ("a", 1),
    ("penny", 1),("two", 1),
    ("a", 1), ("penny", 1),
    ("hot", 1), ("cross", 1),
    ("buns", 1)]
```
\endgroup

```


  Group by key:
```
\begingroup\tiny\color{blue}
```
    [("a", [1, 1]),
    ("buns", [1]),
    ("cross", [1]),
    ("hot", [1]),
    ("one", [1]),
    ("penny", [1, 1]),
    ("two", [1])]
```
\endgroup
```
  Reduce:
```

\begingroup\tiny\color{blue}
```
    [("one", 1), ("a", 2),
    ("penny", 2),
    ("two", 1),
    ("hot", 1),
    ("cross", 1),
    ("buns", 1)]
```
\endgroup
::::
:::

* MapReduce: Word Count
```graphviz
digraph MapReduce {
    // --- Graph-level settings ---
    graph [
        // rankdir=LR,     // Default is TB (Top-Bottom)
        ranksep=0.5,    // Increase space between columns
        nodesep=1.0,    // Increase space between nodes on the same rank
        splines=false,  // Use straight lines
        bgcolor=white,  // Set background to white
        fontname="Helvetica"
    ];

    // --- Default node settings ---
    node [
        fontname="Helvetica",
        fontsize=14,
        fontcolor=black
    ];

    // --- Default edge settings (for the big dashed arrows) ---
    edge [
        style="dashed,bold",
        arrowhead=none,
        penwidth=6,
        color=black
    ];

    // --- Column Headers ---
    // We create invisible nodes to hold the header text
    node [shape=plaintext, fontsize=16, fontcolor=black];
    input_header  [label=""]; // Empty placeholder for doc, to match group_header
    map_header    [label=<<b>Provided by the<br/>programmer</b>>];
    group_header  [label=""]; // Empty placeholder for alignment
    reduce_header [label=<<b>Provided by the<br/>programmer</b>>];

    
    // --- Node Definitions ---

    // 1. Big Document
    // Converted to an HTML-like label to add ports to each text block
    node [
        shape=box,
        style=solid,
        color=black,
        width=2.5,
        height=3.5,
        align=left
    ];
    doc [
        label=<
<TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" ALIGN="LEFT">
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="part1">The crew of the space<BR/>
shuttle Endeavor recently<BR/>
returned to Earth as<BR/>
ambassadors, harbingers<BR/>
of a new era of space<BR/>
exploration. Scientists at</TD></TR>
  <TR><TD ALIGN="CENTER" HEIGHT="1" VALIGN="MIDDLE"><FONT FACE="monospace" POINT-SIZE="8">------------------------------</FONT></TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="part2">
NASA are saying that the<BR/>
recent assembly of the<BR/>
Dextre bot is the first step<BR/>
in a long-term space-based<BR/>
man/machine partnership.</TD></TR>
  <TR><TD ALIGN="CENTER" HEIGHT="1" VALIGN="MIDDLE"><FONT FACE="monospace" POINT-SIZE="8">------------------------------</FONT></TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="part3">
\"The work we're doing now<BR/>
-- the robotics we're doing<BR/>
-- is what we're going to<BR/>
need ...</TD></TR>
</TABLE>
>,
        xlabel=<<b><font color="darkgreen">Big document</font></b>>
    ];

    // 2. Map Node
    // Data section is now multiple rows, each with a port
    node [
        shape=box,
        style="filled,rounded",
        fillcolor="#d29a9a", // Reddish color
        width=2.5,
        height=3.5,
        fontcolor=black
    ];
    map_node [
        label=<
<TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" ALIGN="LEFT">
  <TR><TD ALIGN="LEFT" CELLPADDING="8" PORT="desc"><B>Map:</B><BR/>Read input<BR/>Produce a set of<BR/>key-value pairs</TD></TR>
  <TR><TD ALIGN="CENTER" HEIGHT="1" VALIGN="MIDDLE"><FONT FACE="monospace" POINT-SIZE="8">--------------------------</FONT></TD></TR> <!-- Simulated dashed line -->
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="m_data1">(The, 1)<BR/>(crew, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="m_data2">(of, 1)<BR/>(the, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="m_data3">(space, 1)<BR/>(shuttle, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="m_data4">(Endeavor, 1)<BR/>(recently, 1)<BR/>....</TD></TR>
</TABLE>
>,
        xlabel=<<b><font color="darkgreen">(key, value)</font></b>>
    ];

    // 3. Group by key Node
    // Data section is now multiple rows, each with a port
    node [
        shape=box,
        style="filled,rounded",
        fillcolor="#8c96c6", // Bluish color
        width=2.5,
        height=3.5,
        fontcolor=black
    ];
    group_node [
        label=<
<TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" ALIGN="LEFT">
  <TR><TD ALIGN="LEFT" CELLPADDING="8" PORT="desc"><B>Group by<BR/>key:</B><BR/>Collect all pairs<BR/>with same key</TD></TR>
  <TR><TD ALIGN="CENTER" HEIGHT="1" VALIGN="MIDDLE"><FONT FACE="monospace" POINT-SIZE="8">--------------------------</FONT></TD></TR> <!-- Simulated dashed line -->
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_crew">(crew, [1, 1])</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_space">(space, [1])</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_the">(the, [1, 1, 1])</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_shuttle">(shuttle, [1])</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_recently">(recently, [1])</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="g_dots">...</TD></TR>
</TABLE>
>,
        xlabel=<<b><font color="darkgreen">(key, value)</font></b>>
    ];

    // 4. Reduce Node
    // Data section is now multiple rows, each with a port
    node [
        shape=box,
        style="filled,rounded",
        fillcolor="#a68cc6", // Purplish color
        width=2.5,
        height=3.5,
        fontcolor=black
    ];
    reduce_node [
        label=<
<TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" ALIGN="LEFT">
  <TR><TD ALIGN="LEFT" CELLPADDING="8" PORT="desc"><B>Reduce:</B><BR/>Collect all values<BR/>belonging to the<BR/>key and output</TD></TR>
  <TR><TD ALIGN="CENTER" HEIGHT="1" VALIGN="MIDDLE"><FONT FACE="monospace" POINT-SIZE="8">--------------------------</FONT></TD></TR> <!-- Simulated dashed line -->
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_crew">(crew, 2)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_space">(space, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_the">(the, 3)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_shuttle">(shuttle, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_recently">(recently, 1)</TD></TR>
  <TR><TD ALIGN="LEFT" CELLPADDING="4" PORT="r_dots">...</TD></TR>
</TABLE>
>,
        xlabel=<<b><font color="darkgreen">(key, value)</font></b>>
    ];

    
    // --- Layout and Edges ---

    // Place all headers on the same top rank
    { rank=same; input_header; map_header; group_header; reduce_header; }

    // Place all main nodes on the same rank below the headers
    { rank=same; doc; map_node; group_node; reduce_node; }

    // Use invisible edges to align headers vertically with their nodes
    input_header -> doc          [style=invis];
    map_header -> map_node       [style=invis];
    group_header -> group_node   [style=invis];
    reduce_header -> reduce_node [style=invis];

    // Define the main data flow
    // 1. Big dashed arrow from Doc to Map
    // This edge uses the default style (dashed, bold) defined at the top.
    doc:part1 -> map_node:desc; // Connects from a part of the doc to the map description

    // 2. New dotted lines from Map to Group
    // We change the edge style *only* for these connections
    edge [style=dotted, color=black, penwidth=2, arrowhead=normal, arrowsize=0.8, constraint=false];
    map_node:m_data1 -> group_node:g_crew;     // (crew, 1) -> (crew, [1, 1])
    map_node:m_data2 -> group_node:g_the;      // (the, 1) -> (the, [1, 1, 1])
    map_node:m_data3 -> group_node:g_space;    // (space, 1) -> (space, [1])
    map_node:m_data3 -> group_node:g_shuttle;  // (shuttle, 1) -> (shuttle, [1])
    map_node:m_data4 -> group_node:g_recently; // (recently, 1) -> (recently, [1])
    // Note: (The, 1) and (of, 1) don't have obvious matches in the group list, so they are not explicitly connected

    // 3. New dotted lines from Group to Reduce
    // These edges will *inherit* the dotted style from the change above
    group_node:g_crew -> reduce_node:r_crew;
    group_node:g_space -> reduce_node:r_space;
    group_node:g_the -> reduce_node:r_the;
    group_node:g_shuttle -> reduce_node:r_shuttle;
    group_node:g_recently -> reduce_node:r_recently;
}
```

* MapReduce: Map Step
:::columns
::::{.column width=40%}
\vspace{1cm}
- **map(values: List):**
\begingroup\footnotesize
```python
  # values: words in document
  for word in values:
		emit(word, 1)
```
\endgroup

- **map()** needs to process all the values 
- can output 0 or more tuples for each input
::::
::::{.column width=60%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_28_image_1.png){width=90%}
::::
:::


* MapReduce: Reduce Step
```python
reduce(key: word, values: List[int]):
   # key: a word
   # value: an iterator over counts
   result = 0
   for count in values:
	result += count
   emit(key, result)
```
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_29_image_1.png)



* MapReduce: Interfaces
- Input: read key-value pairs **List[Tuple[k, v]]**
- Programmer specifies two methods map and reduce

- **Map(Tuple[k, v]) → List[Tuple[k, v]]**
  - Take a key-value pair and output a set of key-value pairs
    - E.g., key is a file, value is the number of occurrences
    - "One a penny" → [("One", 1), ("a", 1), ("penny", 1)]
  - There is one **Map** call for every *(*k*, v) pair*
- **GroupBy(List[Tuple[k, v]]) → List[Tuple[k, List[v]]]]**
  - Group and optionally sort all the records with the reduce key
- **Reduce(Tuple[k, List[v]]) → Tuple[k, v]**
  - All values *v'* with same key *k'* are reduced together
  - There is one **Reduce** call per unique key *k'*
- Output: write key-value pairs **List[Tuple[k, v]]**

* MapReduce: Log Processing
:::columns
::::{.column width=60%}
\footnotesize
- Log file recording access to a website with format
```
date, hour, filename
```
- **Goal**: find how many times each files is accessed during Feb 2013
- **Input**
  - Read the file and split into lines
- **Map**
  - Parse each line into the 3 fields
  - If the date is in the required interval emit(dir_name, 1)
- **GroupBy**
  - The reduce key is the filename
  - Accumulate all the (key, value) with the same filename
- **Reduce**
  - Add the values for each list of (key, value) since they have the same filename
  - Output the number of access to each file
- **Output**
  - Write results on disk separated by newline

::::
::::{.column width=40%}
\footnotesize
**After Input**
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_31_image_1.png)

**After Map**
```
[`/slide-dir/11.ppt`, 1), ...)]
```
\vspace{0.2cm}
**After GroupBy**

```
[`/slide_dir/11.ppt`, 1), ...,
(` /slide-dir/12.ppt`, [1, 1]), ...)]
```
\vspace{0.2cm}
**After Reduce**
```
[`/slide_dir/11.ppt`, 1), ...,
(`/slide-dir/12.ppt`, 2), ...)]
```
\vspace{0.2cm}
**Output**
```
/slide_dir/11.ppt 1
...
/slide-dir/12.ppt 2
...
```
::::
:::

* MapReduce: Data Flow
```
Focusing on MapReduce functionality / flow of the data to 
expose the parallelism
```
:::columns
::::{.column width=60%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_32_image_1.png){width=110%}
::::
::::{.column width=40%}
- **Input**
- **Map**
  - mki = map keys
  - mvi = map input values
- **GroupBy**
  - Shuffle / collect the data
- **Reduce**
  - rki = reduce keys
  - rvi** = reduce input values
  - Reduce outputs are not shown
::::
:::
\vspace{0.25cm}
:::columns
::::{.column width=10%}
**Input**
::::
::::{.column width=15%}
**Map**
::::
::::{.column width=10%}
**GroupBy**
::::
::::{.column width=10%}
**Reduce**
::::
::::{.column width=40%}
```
.
```
::::
:::

* MapReduce: Parallel Data Flow
:::columns
::::{.column width=55%}
\footnotesize
- **User program** specifies map/reduce code
- **Input data** is partitioned across multiple machines (HDFS)
- **Master** node sends copies of the code to all computing nodes
- **Map**
  - *n* data chunks to process
  - Functions executed in parallel on multiple *k* machines
  - Output data from *Map* is saved on disk
- **GroupBy / Sort**
  - Output data from *Map* is sorted and partitioned based on reduce key
  - Different files are created for each *Reduce* task
- **Reduce**
  - Functions executed in parallel on multiple machines
  - Each work on some part of the data
  - Output data from Reduce is saved on disk
- **Write to disk**

::::
::::{.column width=50%}

![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_33_image_1.png)

- All operations use HDFS as storage
- Machines are reused for multiple computations (Map, GroupBy, Reduce) at different times
::::
:::


* Master Node Responsibilities
:::columns
::::{.column width=50%}
- **Master node takes care of coordination**
  - Each task has status (idle, in-progress, completed)
  - Idle tasks get scheduled as workers become available
  - When a *Map task* completes, it sends the Master the location and sizes of its intermediate files
  - Master pushes this info to *Reduce tasks*
  - Reduce tasks become idle and can get scheduled
- **Master node pings workers periodically to detect failures**
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_34_image_1.png)
::::
:::

* Dealing with Failures
- **Map worker failure**
  - Failed map tasks are reset to idle (i.e., back in the queue for execution)
  - Reduce workers are notified when task is rescheduled on another worker
- **Reduce worker failure**
  - Only in-progress tasks are reset to idle
  - Reduce task is restarted
- **Master failure**
  - MapReduce task is aborted
  - Client is notified

* How many Map and Reduce jobs?
- **M** **map tasks**
- **R** **reduce tasks**
- **N** **worker nodes**
- Rules of thumb
  - *M* >> *N*
    - Pros: Improve dynamic load balancing, Speed up recovery from worker failures
    - Cons: More communication between *Master* and *Worker Nodes*, Lots of smaller files
  - *R* > *N*
    - Usually *R* < *M*, Output is spread across fewer files

* Refinements: Backup Tasks
- **Problem**
  - Slow workers significantly lengthen the job completion time
  - Slow workers due to:
    - Older processor
    - Not enough RAM
    - Other jobs on the machine
    - Bad disks
    - OS thrashing / virtual memory hell
- **Solution**
  - Near the end of Map / Reduce phase
    - Spawn backup copies of tasks
    - Whichever one finishes first "wins"
- **Result**
  - Shorten job completion time

* Refinement: Combiners
:::columns
::::{.column width=60%}
- **Problem**
  - Often a *Map* task produces many pairs for the same key *k*
```
  [(k1, v1), (k1, v2), ...]
```
  - E.g., common words in the word count example
  - Increase complexity of the GroupBy stage
- **Solution**
  - Pre-aggregate values in the *Map* with a *Combine*
```
  [k1, (v1, v2, ...), k2, ([...])]
```
  - *Combine* is usually the same as the *Reduce* function
  - Works only if *Reduce* function is commutative and associative
- **Result**
  - Better data locality
  - Less shuffling and reordering
  - Less network / disk traffic
::::
::::{.column width=50%}
\vspace{1cm}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_38_image_1.gif)
::::
:::

* Refinement: Partition Function
- **Problem**
  - Sometimes users want to control how keys get partitioned
  - Inputs to *Map* tasks are created by contiguous splits of input file
  - MapReduce uses a default partition function
**hash(key) mod R**
  - Reduce needs to ensure that records with the same intermediate key end up at the same worker
- **Solution**
  - Sometimes useful to override the hash function:
  - E.g., **hash(hostname(URL)) mod R** ensures URLs from a host end up in the same output file

* UMD DATA605 - Big Data System
- Storing and Computing Big Data
- MapReduce Framework
- **(Apache) Hadoop**
- Algorithms
- MapReduce vs DBs  

* Implementations of MapReduce
- **Google**
  - Not available outside Google
- **Hadoop**
  - Website
  - An open-source implementation in Java
  - Uses HDFS for stable storage
  - Hadoop Wiki
    - Introduction, Getting Started, Map/Reduce Overview
- **Amazon Elastic MapReduce (EMR)**
  - Website
  - Hadoop MapReduce running on Amazon EC2
  - Can also run Spark, HBase, Hive, ...
- **Spark**
- **Dask**

* MapReduce: Hadoop
- Hadoop is an open-source implementation of MapReduce
- Functionalities
  - Partition the input data (HDFS)
  - Input adapters
    - E.g., HBase, MongoDB, Cassandra, Amazon Dynamo
  - Schedule program's execution across a set of machines
  - Handle machine failures
  - Manage inter-machine communication
  - Perform the *GroupByKey* step
  - Output adapters
    - E.g., Avro, ORC, Parquet
  - Schedule multiple *MapReduce* jobs

![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_42_image_1.png){width=80%}

* Data Flow
- Input, intermediate, final outputs are stored in a distributed file system (HDFS)
  - Every operation in Hadoop goes from disk to disk
- Adapters to read / partition the data in chunks
- Scheduler tries to schedule map tasks "close" to physical storage location of input data
  - Intermediate results (e.g., GroupBy) are stored on local FS of Map and Reduce workers
- Output is often input to another MapReduce task

* Input Data
  - **InputData** stores the data for a **MapTask** typically in a distributed file system (e.g., HDFS)
  - The format of input data is arbitrary
    - Line-based log files
    - Binary files
    - Multi-line input records
    - Something else (E.g., an SQL database)

![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_44_image_1.png)

* InputFormat
  - **InputFormat** class reads and splits up the input files
  - Select the files that should be used for input
  - Defines the **InputSplits** that break a file
  - Provides a factory for **RecordReaders** objects that read the file

\footnotesize
| InputFormat | Description | Key | Value |
|-------------|-------------|-----|-------|
| TextInputFormat | Default format; reads lines of text files | The byte offset of the line | The line contents |
| KeyValueInputFormat | Parses lines into (K, V) pairs | Everything up to the first tab character | The remainder of the line |
| SequenceFileInputFormat | A Hadoop-specific high-performance binary format | User-defined | User-defined |
\vspace{-0.75cm}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_45_image_1.png){width=60%}



* InputSplit
  - **InputSplit** describes a unit of work that comprises a single **MapTask**
    - By default, the **InputFormat** breaks a file up into 64MB splits
  - By dividing the file into splits
    - Each **MapTask** corresponds to a single input split
    - Several **MapTask**s to operate on a single file in parallel

![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_46_image_1.png)

* RecordReader
- The **InputSplit** defines a slice of work but does not describe how to access it
- The **RecordReader** class
  - Loads data from its source and converts it into **(K, V) pairs** suitable for reading by **MapTasks**
  - Is invoked repeatedly on the input until the entire **InputSplit** is consumed
  - Each invocation leads to a call of the map function defined by the programmer

![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_47_image_1.png)

* OutputFormat

:::columns
::::{.column width=70%}
- The **OutputFormat** class
  - defines the way (K,V) pairs produced by **Reducers** are written to output files
  - write to files on the local disk or in HDFS in different formats

| OutputFormat | Description |
|--------------|-------------|
| TextOutputFormat | Default; writes lines in "key \t value" format |
| SequenceFileOutputFormat | Writes binary files suitable for reading into subsequent MapReduce jobs |
| NullOutputFormat | Generates no output files |
::::
::::{.column width=30%}
```graphviz
digraph G {
    rankdir=TB;
    node [shape=box, style=filled, fontname="Helvetica", margin=0.1];

    // Define Nodes
    label_hdfs [label="Files loaded from local HDFS store", shape=plaintext, style="", fontcolor=black];
    
    file [label="file\nfile", shape=cylinder, style=filled, fillcolor="gray", color="blue", peripheries=1, fontcolor=black];
    
    input [label="InputFormat", fillcolor="#CC0000", fontcolor=white];
    
    split1 [label="Split", fillcolor="#F2C6C6", fontcolor=black];
    split2 [label="Split", fillcolor="#F2C6C6", fontcolor=black];
    split3 [label="Split", fillcolor="#F2C6C6", fontcolor=black];
    
    rr1 [label="RR", fillcolor="#336699", fontcolor=white];
    rr2 [label="RR", fillcolor="#336699", fontcolor=white];
    rr3 [label="RR", fillcolor="#336699", fontcolor=white];
    
    map1 [label="Map", fillcolor="blue", fontcolor=white];
    map2 [label="Map", fillcolor="blue", fontcolor=white];
    map3 [label="Map", fillcolor="blue", fontcolor=white];
    
    partitioner [label="Partitioner", fillcolor="gold", fontcolor=black];
    sort [label="Sort", fillcolor="purple", fontcolor=white];
    reduce [label="Reduce", fillcolor="red", fontcolor=white];
    output [label="OutputFormat", fillcolor="#009933", fontcolor=white];

    // Define Edges (Arrows)
    label_hdfs -> input;
    file -> input;
    
    input -> split1;
    input -> split2;
    input -> split3;

    split1 -> rr1 [dir=both];
    split2 -> rr2 [dir=both];
    split3 -> rr3 [dir=both];

    rr1 -> map1;
    rr2 -> map2;
    rr3 -> map3;

    {map1, map2, map3} -> partitioner;
    
    partitioner -> sort -> reduce -> output;

    // Define Ranks for horizontal alignment
    { rank=same; split1; split2; split3; }
    { rank=same; rr1; rr2; rr3; }
    { rank=same; map1; map2; map3; }
}
```
::::
:::

* UMD DATA605 - Big Data System
- Storing and Computing Big Data
- MapReduce Framework
- (Apache) Hadoop
- **Algorithms**
- MapReduce vs DBs  

* MapReduce: Applications

- Major classes of applications
  - Text tokenization, indexing, and search
  - Processing of large data structures
  - Data mining and machine learning
  - Link analysis and graph processing

* Example: Language Model

- Statistical machine translation
  - Need to count number of times every 5-word sequence occurs in a large corpus of documents
- Large Language Models
  - OpenAI GPT
- Very easy with MapReduce
  - Map
    - Extract (5-word sequence, count) from document
  - Reduce
    - Combine the counts

* Cost Measures for Distributed Algorithms

- **Quantify the cost of a parallel algorithm in terms of:**
- Communication cost
  - = total I/O of all processes
  - Related to disk usage as well
- Elapsed communication cost
  - = max I/O along any path (critical path)
- Elapsed computation cost
  - = end-to-end running time of algorithm
  - It is the wall-clock time using parallelism
- **Total cost**
  - = what you pay as rent to your "friendly" neighborhood cloud provider
  - CPU + disk + I/O used
  - Either CPU, disk, I/O cost dominates $\to$ ignore the others
- **In this case, the big-O notation is not the most useful**
  - The actual cost matters and not the asymptotic cost!
  - Multiplicative constant matters
  - Adding more machines is always an option

* MapReduce Cost Measures

- **For a `map-reduce` algorithm:**
- **Communication cost**
  - = total I/O of all processes
  - input file size
  - + 2 × (sum of the sizes of all files passed from Map processes to Reduce processes) [You need to write and read back the data]
  - + the sum of the output sizes of the Reduce processes
- **Elapsed communication cost**
  - = max of I/O along any path
  - sum of the largest input + output for any Map process, plus the same for any Reduce process
- **Elapsed computation cost**
  - = end-to-end running time of algorithm
  - Ideally all Map and Reduce processes end at the same time
  - Workload is "perfectly balanced"

* Example: Join By MapReduce

:::columns
::::{.column width=75%}
- Compute the natural join **R(A,B) $\bowtie$ S(B,C)** joining on column **B**
- **R** and **S** are stored in files as pairs **(a, b)** or **(b, c)**
- Use a hash function *h* from B-values to *h(b)* in [1, ..., k]
- **Map task**
  - Transform an input tuple *R(a, b)* into key-value pair *(h(b), (a, R))*
  - Each input tuple *S(b, c)* $\to$ *(h(b), (c, S))*
- **GroupBy task**
  - Each key-value pair with key *b* to is sent to Reduce task *h(b)*
  - Hadoop does this automatically; just tell it what *h* is
- **Reduce task**
  - Matches all the pairs (b, (a, R)) with all (b, (c, S)) to get (a, b, c)
  - Output (a, c)
::::
::::{.column width=25%}

:::columns
::::{.column width=5%}
\vspace{1.5cm}
R
::::
::::{.column width=15%}
| A    | B    |
|------|------|
| a1   | b1   |
| a2   | b1   |
| a3   | b2   |
| a4   | b3   |
::::
::::{.column width=5%}
\vspace{1.5cm}
$\bowtie$
::::
:::



:::columns
::::{.column width=5%}
\vspace{1.5cm}
S
::::
::::{.column width=15%}
| B    | C    |
|------|------|
| b2   | c1   |
| b2   | c2   |
| b3   | c3   |
::::
::::{.column width=5%}
\vspace{1.5cm}
```
=
```
::::
:::



:::columns
::::{.column width=5%}

::::
::::{.column width=15%}
| A    | C    |
|------|------|
| a3   | c1   |
| a3   | c2   |
| a4   | c3   |
::::
::::{.column width=5%}

::::
:::


::::
:::

* Cost of MapReduce Join

- **Total communication cost**
  - = total I/O of all processes
  - = O(|R|+|S|+|R  bowtieS|)
  - You need to read all the data and then write the result
  - It doesn't matter how you split the computation
- **Elapsed communication cost**
  - We put a limit **s** on the amount of input or output that any one process can have, e.g.,
    - What fits in main memory
    - What fits on local disk
  - = O(s)
  - We're going to pick the number of Map and Reduce processes so that the I/O limit **s** is respected
- **Computation cost**
  - = O(|R|+|S|+|R $\bowtie$ S|)
  - Using proper indexes there is no shuffle
  - So computation cost is like communication cost

* UMD DATA605 - Big Data System
- Storing and Computing Big Data
- MapReduce Framework
- (Apache) Hadoop
- Algorithms
- **MapReduce vs DBs**  

* History
- Abstract ideas about MapReduce have been known before Google's MapReduce paper
- **The strength of MapReduce comes from simplicity, ease of use, and performance**
  - Declarative design
  - User specifies what is to be done, not how many machines to use, etc...
  - Many times commercial success comes from making something simple to use
- MapReduce can be implemented using user-defined aggregates in PostgreSQL quite easily
  - See MapReduce and Parallel DBMSs by Stonebraker et al., 2010
- No database system can come close to the performance of MapReduce infrastructure
  - E.g., RDBMSs
  - Can't scale to that degree
  - Are not as fault-tolerant
  - Designed to support ACID
    - Most MapReduce applications don't care about ACID consistency



* History
- **MapReduce**
- Is very good at doing what it was designed for
  - If the application maps well to MapReduce, one can achieve optimal theoretical speed-up
- May not be ideal for more complex tasks
  - E.g., no notion of "query optimization", e.g., operator order optimization
  - The sequence of MapReduce tasks makes it procedural within a single machine
- Assumes a single input
  - E.g., joins are tricky to do, but doable
- Much work in recent years on extending the basic MapReduce functionality, e.g.,
- Hadoop Zoo
- E.g., Spark, Dask, Ray

* Hadoop Ecosystem (aka Hadoop Zoo)
:::columns
::::{.column width=50%}
- Pig
  - High-level data-flow language and execution framework for parallel computation
- HBase
  - Scalable, distributed database
  - Supports structured data storage for large tables (like Google BigTable)
- Cassandra
  - Scalable multi-master database with no single points of failure
- Hive
  - Data warehouse infrastructure
  - Provide data summarization and ad-hoc querying
- ZooKeeper
High-performance coordination service for distributed applications
- YARN, Kafka, Storm, Spark, Solr, ...
::::
::::{.column width=50%}
\vspace{1cm}
![](data605/lectures_source/images/lecture_8_1/lec_8_1_slide_59_image_1.png)
::::
:::