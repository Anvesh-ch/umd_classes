// notes_to_pdf.py --input lectures_source/Lesson9-Causal_inference.txt --output tmp.pdf -t slides
// helpers_root/./dev_scripts_helpers/documentation/render_images.py -i lectures_source/Lesson9-Causal_inference.txt -o lectures_source/Lesson9-Causal_inference.out.txt

::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}
\begingroup \Large
**$$\text{\blue{9.2: Causal Inference}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese - [](gsaggese@umd.edu)

**References**:
  - AIMA
  - The Book of Why

# Causal Networks

// ## 13.5 Causal networks (p. 462)

* (Non-Causal) Bayesian Networks
- **Bayesian networks** represent a joint distribution function
  - The direction of the arrow represent **conditional dependence** (not
    causality)
    - $A \to B$ requires to estimate $\Pr(A | B)$
::: columns
:::: {.column width=60%}

- There are **many possible** _edges_ and _node ordering_ for the same Bayesian
  network
  - E.g., a Bayesian network with $Fire$ and $Smoke$, which are dependent
    - $Fire \to Smoke$
      - Need $\Pr(Fire)$ and $\Pr(Smoke | Fire)$ to compute $\Pr(Fire, Smoke)$
    - $Smoke \to Fire$
      - Need $\Pr(Smoke)$ and $\Pr(Fire | Smoke)$
::::
:::: {.column width=35%}
```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Fire -> Smoke [label="P(Smoke | Fire)"];
}
```

\vspace{1cm}

```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Smoke -> Fire [label="P(Fire | Smoke)"];
}
```
::::
:::

- Different Bayesian networks:
  - Are equivalent and convey the same information
  - Have different difficulties to be estimated

- At the same time, there is an **asymmetry in nature**
  - Extinguishing fire stops smoke
  - Clearing smoke doesn't affect fire

* Causal (Bayesian) Networks
- **Causal networks** are Bayesian networks forbidding **non-causal edges**

- Use judgment based on nature instead of just statistics
  - E.g., from _"Are random variables $Smoke$ and $Fire$ correlated?"_ to _"What
    causes what, $Smoke$ or $Fire$?"_

- **"Dependency in nature"** is like assignment in programming
  - E.g., nature assigns $Smoke$ based on $Fire$:
    - ![](emoji/check_mark.png){ width=12px } $Smoke := f(Fire)$
    - ![](emoji/wrong.png){ width=12px } $Fire := f(Smoke)$

- **Structural equations** describe "assignment mechanism" in causal graphs
  $$
  X_i := f(X_j) \iff X_j \to X_i
  $$

* Causal DAG
- **Causal DAG**
  - _Directed_: Arrows show cause $\rightarrow$ effect
  - _Acyclic_: No feedback loops
    - Causal relationships assume temporal order: cause before effect
    - A cycle implies a variable is both cause and effect of itself

- **Benefits**
  - DAGs encode _causal_ links
  - Reason about interventions and counterfactuals
  - Support explainable AI models
  - Stability in conditional probability estimation

- **Limitations**
  - Requires domain knowledge for structure
  - Assumes all relevant variables included (no hidden confounders)

* Causal Edges are Stable

- **Causal edge** $X \rightarrow Y$ shows direct causal influence of $X$ on $Y$,
  holding other variables constant
  - Captures how manipulating $X$ changes $Y$, not just their covariance

- Causal edges reflect **stable relationship**

  - Mechanistic stability
    - Causal relationships show system function, not just behavior in one dataset
    - E.g., "Temperature $\rightarrow$ ice melting rate" holds true in Alaska and
      Arizona

  - Invariance under interventions
    - If $X$ causes $Y$, intervening on $X$ affects $Y$ consistently, despite
      confounders or context changes

  - Easier estimation through causal modeling
    - Identifying causal direction focuses estimation on effect size (e.g.,
      regression of $Y$ on $X$ under intervention)

  - Reduced sensitivity to sampling and omitted variables
    - Correlations may change with confounder addition or removal
    - True causal edge persists, stable across model specifications

- **Example**: study $Exercise \rightarrow Health$:
  - Correlation may differ in young or elderly populations
  - Causal effect remains stable, as physiological mechanism doesn't change

* Structural Causal Model
- A **Structural Causal Model** (SCM) translates a causal DAG into mathematical
  equations
  - DAGs show structure (variables and arrows)
  - SCMs use equations to define how variables interact

- **Structure of SCMs**
  - Variables $X_1, X_2, ..., X_n$ represent quantities in the system
  - Equations model each variable as a function of its direct causes
  - Formally, $X_i$ is modeled as:
    $$X_i = f_i(Parents(X_i), \varepsilon_i)$$
    where:
    - $Parents(X_i)$ are direct causes of $X_i$
    - $\varepsilon_i$ is an exogenous (external, unobserved) noise term

- **Properties**
  - Explain causal relationships between variables
  - Provide a foundation for causal reasoning and simulation
  - Describe how the world works, not just variable correlations

* Structural Causal Model: Example
::: columns
:::: {.column width=50%}
- **Explanatory variables**
  - You can manipulate or observe when changes are applied
  - E.g., _"does a large cup of coffee before an exam help with a test?"_

- **Outcome variables**
  - Result of the action
  - E.g., _"by how much did the score test improve?"_
::::
:::: {.column width=45%}
```graphviz
digraph SCMExample {
  rankdir=TB;
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  Coffee [label="Large Cup of Coffee", fillcolor="#b3cde3"];
  TestScore [label="Test Score Improvement", fillcolor="#ccebc5"];
  RoomTemp [label="Room Temperature", fillcolor="#decbe4"];

  // Edges
  Coffee -> TestScore;
  RoomTemp -> TestScore [style=dashed];
}
```
::::
:::

- **Unobserved variables**
  - Not seen or more difficult to account
  - E.g., _"temperature of the room makes students sleepy and less alert"_

* Structural Causal Model: Sprinkler Example
::: columns
:::: {.column width=25%}

```graphviz
digraph BayesianFlow {
    rankdir=TD;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    Cloudy        [label="Cloudy",        fillcolor="#b3cde3"];
    Sprinkler     [label="Sprinkler",     fillcolor="#ccebc5"];
    Rain          [label="Rain",          fillcolor="#ccebc5"];
    WetGrass      [label="Wet Grass",     fillcolor="#decbe4"];
    GreenerGrass  [label="Greener Grass", fillcolor="#fed9a6"];

    // Force ranks
    { rank=same; Sprinkler; Rain; }

    // Edges
    Cloudy -> Sprinkler;
    Cloudy -> Rain;
    Sprinkler -> WetGrass;
    Rain -> WetGrass;
    WetGrass -> GreenerGrass;
}
```
::::
:::: {.column width=70%}
- Structural equations for this model:
  \begingroup \small
  $$
  \left\{
  \begin{aligned}
    & C := f_C(\varepsilon_C) \\
    & R := f_R(C, \varepsilon_R) \\
    & S := f_S(C, \varepsilon_S) \\
    & W := f_W(R, S, \varepsilon_W) \\
    & G := f_G(W, \varepsilon_G) \\
  \end{aligned}
  \right.
  $$
  \endgroup

- Unmodeled variables $\varepsilon_x$ represent error terms
  - E.g., $\varepsilon_W$ is another source of wetness besides $Sprinkler$ and $Rain$
    (e.g., $MorningDew$)
- Assume unmodeled variables are exogenous, independent, with a certain
  distribution (prior)
::::
:::

- Express joint distribution of five variables as a product of conditional
  distributions using causal DAG topology:
  \begingroup \small
  $$
  \Pr(C, R, S, W, G) = \Pr(C) \Pr(R|C) \Pr(S|C) \Pr(W|R,S) \Pr(G|W)
  $$
  \endgroup

## #############################################################################
## Variables
## #############################################################################

* Observed Vs. Unobserved Variables
::: columns
:::: {.column width=45%}
- **Observed variables**
  - Aka "measurable" or "visible"
  - Variables directly measured or collected in a dataset
  - E.g.,
    - Education
    - Income
    - Blood pressure
    - Product price

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_ObservedUnobserved_Vertical {
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#ccebc5"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#decbe4"];
    Weather         [label="Weather",         fillcolor="#decbe4"];
    News            [label="News",            fillcolor="#decbe4"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend (top, vertical stack)
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Observed", shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key2 [label="Unobserved",  shape=box, style="rounded,filled", fillcolor="#decbe4"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::

- **Unobserved variables**
  - Aka "latent" or "hidden"
  - Exist but not measured or included in data
  - E.g.,
    - Natural talent
    - Motivation
    - Company culture
  - Ignoring unobserved variables distorts causal relationships
    - Observed: $IceCreamSales$ and $DrowningRates$
    - Unobserved: $Temperature$
    - Misleading conclusion: $IceCream$ causes $Drowning$

* Endogenous Vs. Exogenous Variables
::: columns
:::: {.column width=45%}
- **Endogenous variables**
  - Values determined _within_ the model
    - Dependent on other variables in the system
  - Represent system's internal behavior and outcomes
  - E.g.,
    - Motivation
    - Income

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#fed9a6"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#fed9a6"];
    Motivation      [label="Motivation",      fillcolor="#b3cde3"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend at top
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Endogenous", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::
- **Exogenous variables**
  - Originate _outside_ the system being modeled
    - Not caused by other variables in the model
  - Represent background conditions or external shocks
  - E.g.,
    - Natural talent
    - Economic policy
    - Weather
    - News

* Endo / Exogenous, Observed / Unobserved Vars
::: columns
:::: {.column width=40%}
- In **Structural Causal Models**
  $$
  X_i = f_i(Parents(X_i), \varepsilon_i)
  $$
  where:
  - $X_i$: endogenous
  - $\varepsilon_i$: exogenous noise

- **Typically**
  - _Endogenous variables_: focus for prediction and intervention
  - _Exogenous variables_: capture randomness or unknown external factors

::::
:::: {.column width=55%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical layout for legend

        key1 [label="Endogenous / Observed", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous / Observed",  shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key3 [label="Endogenous / Unobserved", shape=box, style="rounded,filled", fillcolor="#decbe4"];
        key4 [label="Exogenous / Unobserved",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
        key2 -> key3 [style=invis];
        key3 -> key4 [style=invis];
    }
}
```

\begingroup \scriptsize
| **Variable Type**        | **Observability**   | **Example**          |
|---------------------------|---------------------|---------------------|
| Endogenous               | Observed            | Income               |
| Exogenous                | Observed            | Education            |
| Endogenous               | Unobserved          | Motivation           |
| Exogenous                | Unobserved          | Natural Talent       |
\endgroup

::::
:::

## Intervention

* Estimating Causal Effects

- **Goal**: Determine the causal effect of a treatment variable (aka
  intervention) $T$ on an outcome $Y$

::: columns
:::: {.column width=60%}
- **Example:**
    - $T$ = _"takes drug"_
    - $Y$ = _"recovers"_
    - $C$ = _"overall health"_
- Healthier people may take medicine and recover faster $\implies$ correlation
  without causation

- In **observational data**
  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
  - $C$ creates _spurious correlation_ between $T$ and $Y$
::::
:::: {.column width=35%}
```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="affects"];
    C -> Y [label="affects"];
    T -> Y [label="causal effect?"];
}
```
::::
:::

- **Problem**
  - There is a "backdoor path" $Treatment \leftarrow Confounder \rightarrow
    Outcome$

* Frontdoor and Backdoor Paths: Intuition
::: columns
:::: {.column width=55%}

- A **backdoor path** is any path from $T$ to $Y$ starting with an arrow into
  $T$
  - E.g., $T \leftarrow C \rightarrow Y$
  - Interpretation:
    - $C$ is a common cause of $T$ and $Y$, confounding their relationship
    - Controlling (conditioning) for $C$ blocks the backdoor path, identifying
      the causal effect of $T$ on $Y$
::::
:::: {.column width=40%}

```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)",fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
    C -> Y [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
    T -> Y [label="Frontdoor", color="#1E8449", fontcolor="#1E8449", penwidth=2.5];
}
```

::::
:::

- A **frontdoor path** goes directly or indirectly from $T$ to $Y$ through
  mediators, following causal flow
  - E.g., $T \rightarrow Y$
  - Interpretation:
    - Direct causal path of interest
    - No mediators, so front-door path is direct causal effect of $T$ on $Y$

* Randomized Controlled Trials (RCTs)

- **Randomized Controlled Trial** is an experimental study to assess causal
  effect of an intervention or treatment
  - Determine whether an intervention causes an effect, not just associated with
    it
  - Eliminate selection bias and confounding variables through randomization

- **Key Components**
  - _Randomization_: ensures groups are statistically equivalent at baseline
  - _Control Group_: receives a placebo or standard treatment
  - _Blinding_: participants and/or researchers do not know the assignment to
    avoid bias
  - _Outcome Measurement_: pre-defined metrics assess the intervention's effect

- **Example**: testing a new drug
  - Treatment group receives the new drug
  - Control group receives a placebo
  - Compare recovery rates after a fixed period

- **Pros**
  - Provides clear causal inference due to randomization

- **Cons**
  - Expensive and time-consuming
  - Ethical or practical constraints may prevent randomization

* RCTs Solve the Problem of Confounders
::: columns
:::: {.column width=65%}
- In **observational data**
  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
  - $C$ creates _spurious correlation_ between $T$ and $Y$
::::
:::: {.column width=30%}
```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="affects"];
    C -> Y [label="affects"];
    T -> Y [label="causal effect?"];
}
```
::::
:::

\vspace{0.5cm}

::: columns
:::: {.column width=55%}
- In **experimental settings**
  - Randomization ($R$) breaks link between $C$ and $T$
  - Random assignment prevents influence on both treatment and outcome
  - $T$ is independent of $C$: $T \perp C$
  - Only open path between $T$ and $Y$ is causal path $T \rightarrow Y$
::::
:::: {.column width=40%}
```graphviz
digraph RCT {
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  rankdir=TD;

  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Node styles
  R [label="Randomization", shape=diamond, fillcolor="#C6A6F4"];
  T [label="Treatment (T)", fillcolor="#A6C8F4"];
  Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
  C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];

  // Edges
  R -> T [label="assigns"];
  T -> Y [label="causal effect!"];
  C -> Y [label="affects"];
}
```
::::
:::

* Causal Graphs and Interventions
- **Observing correlations** between variables _does not reveal causality_
  - $\Pr(Y | T)$ confounds direct and indirect influences

- **Randomized Controlled Trials** provide the _gold standard_ for causal
  inference
  - Randomization breaks all back-door (confounding) paths
  - RCTs are expensive, slow, or ethically impossible

- **Alternative solution**
  - Can we estimate the _causal effect_ from _observational data alone_?
  - Under _what conditions_ and using _which variables_?

- **Idea**: Identify and condition on the right _confounders_ to:
  - Block spurious associations between $T$ and $Y$
  - Recover the true causal effect $\Pr(Y | do(T))$

* Intervention in Structural Equations
- **Purpose of Structural Equations**
  - Capture causal mechanisms among variables
  - Predict impact of external interventions

- **Effect of Intervention** $do(X_j = x_j)$
  - Original equation:
    $$
    X_j = f_j(Parents(X_j), \varepsilon_j)
    $$
  - Modified by intervention:
    $$
    X_j = x_j \text{ (fixed value)}
    $$
  - "Mutilate" causal network by _removing incoming edges_ to $X_j$
  - Recompute joint distribution of all variables using modified structure

- **Intuition**
  - $do$-operator enforces variable's value externally, breaking causal
    dependencies
  - Enables reasoning about "what would happen if...?" scenarios

* Adjustment Formula in Causal Networks

- **Goal**
  - Estimate causal effect of intervention $do(X_j = x_{jk})$ on another variable $X_i$

- **The Adjustment Formula**
  - Derived from the post-intervention joint distribution:
    $$
    \Pr(X_i = x_i | do(X_j = x_{j}^*)) =
    \sum_{Parents(X_j)}
    \Pr(x_i | x_{j}^*, Parents(X_j))
    \Pr(Parents(X_j))
    $$
  - The mechanism for $X_j$ is _removed_: it is treated as a fixed cause, not a
    random variable

- **Interpretation**
  - Computes a _weighted average_ of effects of $X_j$ and its parents on $X_i$
  - Weights come from prior probabilities of the parents’ values

- **Back-Door Criterion**
  - A set $Z$ is a valid adjustment set if it blocks _all back-door paths_ from
    $X_j$ to $X_i$
  - Ensures $X_i \perp \text{Parents}(X_j) | X_j, Z$

- **Why It Matters**
  - Enables causal inference from observational data
  - Estimate treatment and policy effects _without randomized trials_

* Backdoor Criterion: Definition
- A set of variables $Z$ satisfies the **backdoor criterion** for variables $X$
  (cause) and $Y$ (effect) in a causal graph if:
  1. **No element of $Z$ is a descendant of $X$**
     - Ensures $Z$ does not "block" part of the causal effect of $X$ on $Y$
     - Descendants of $X$ may carry information about the causal effect and
       should not be controlled for
  2. **$Z$ blocks every path between $X$ and $Y$ containing an arrow into $X$**
     - These paths are _backdoor paths_, representing potential confounding
       influences
     - Blocking them ensures any remaining association between $X$ and $Y$ is
       causal, not spurious

```graphviz[width=70%]
digraph Backdoor_Criterion {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    X [label="X (Cause)", fillcolor="#A6C8F4"];
    Y [label="Y (Effect)", fillcolor="#B2E2B2"];
    Z [label="Z (Control)", fillcolor="#FFD1A6"];
    U [label="U (Confounder)", fillcolor="#C6A6F4"];
    DescX [label="Descendant of X", fillcolor="#F4A6A6"];

    // Force ranks
    { rank=same; X; Y; Z}

    // Edges
    X -> Y [label="Causal Path", color="#4A90E2", penwidth=2];
    U -> X [label="Backdoor", color="#7E57C2", style=dashed];
    U -> Y [color="#7E57C2", style=dashed];
    Z -> X [label="Confounding", color="#FFA500"];
    Z -> Y [color="#FFA500"];
    X -> DescX [label=" Causal Descendant", color="#E57373"];
}
```

* Backdoor Criterion: Intuition
- **Intuition**:
  - The goal is to isolate the causal effect of $X$ on $Y$ by eliminating
    *confounding bias*
  - Controlling for an appropriate set $Z$ makes the relationship between $X$ and
    $Y$ as if $X$ were randomly assigned

- **Application**:
  - When $Z$ satisfies the backdoor criterion, we can estimate causal effects
    from **observational data** (without experiments)
  - The causal effect can be computed using:
    $$
    \Pr(Y | do(X)) = \sum_z \Pr(Y | X, Z=z) P(Z=z)
    $$

* Intervention: Sprinkler Example
::: columns
:::: {.column width=70%}
- "Intervene" by turning the sprinkler on
  - In do-calculus $do(Sprinkler = T)$
  - Sprinkler variable $s$ is independent of cloudy day $c$

- Structural equations after intervention:
  \begingroup \small
  $$
  \left\{
  \begin{aligned}
  & C := f_C(\varepsilon_C) \\
  & R := f_R(C, \varepsilon_R) \\
  & S := True \\
  & W := f_W(R, S, \varepsilon_W) \\
  & G := f_G(W, \varepsilon_G) \\
  \end{aligned}
  \right.
  $$
  \endgroup

::::
:::: {.column width=25%}

```graphviz
digraph GrassBayes {
    rankdir=TB;
    nodesep=1.0;
    ranksep=0.75;
    splines=true;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Nodes with custom colors
    Cloudy        [label="Cloudy",        fillcolor="#c6dbef"];
    Rain          [label="Rain",          fillcolor="#c6dbef"];
    Sprinkler     [label="Sprinkler\n= True", fillcolor="#e5d5e7"];
    WetGrass      [label="WetGrass",      fillcolor="#c6dbef"];
    GreenerGrass  [label="GreenerGrass",  fillcolor="#c6dbef"];

    // Edges
    Cloudy -> Rain;
    Sprinkler -> WetGrass;
    Rain -> WetGrass;
    WetGrass -> GreenerGrass;
}
```

::::
:::
- $\Pr(S|C) = 1$ and $\Pr(W|R,S) = \Pr(W|R,S=T)$ and the joint probability
  becomes:
  \begingroup \small
  $$
  \Pr(C, R, W, G | do(S=True)) = \Pr(C) \Pr(R|C) \Pr(W|R,S=True) \Pr(G|W)
  $$
  \endgroup

- Only descendants of manipulated variable $Sprinkler$ are affected

* Intervention vs. Observation in Causal Models
- **Intervention** conceptually _breaks_ normal causal dependencies
  - Intervening on $Sprinkler$ removes causal link from $Weather$ to $Sprinkler$
  - After intervention, causal graph excludes arrow $Weather \to Sprinkler$
  - $Weather$ and $Sprinkler$ become independent under intervention

- **Observation vs. Intervention**
  - **Observation**: seeing $Sprinkler = T$
    - Expressed as $\Pr(\cdot | Sprinkler = T)$
    - Reflects _passive observation_ — sprinkler on provides information about
      weather
    - Since $Weather$ influences $Sprinkler$, observing $Sprinkler = T$ makes it
      _less likely_ $Weather$ is cloudy
  - **Intervention**: forcing $Sprinkler = T$
    - Expressed as $\Pr(\cdot | do(Sprinkler = T))$
    - _Active manipulation_ — set sprinkler on regardless of weather
    - Causal link from $Weather$ to $Sprinkler$ is cut, weather distribution
      remains unchanged

- **Key intuition**
  - Observation $\rightarrow$ correlation (information flows along causal links)
  - Intervention $\rightarrow$ causation (links into manipulated variable are
    removed)
  - Thus, $\Pr(Weather | Sprinkler = T) \neq \Pr(Weather | do(Sprinkler = T))$

* Controlling for a Variable in Causal Analysis

- **Definition**
  - To _control_ a variable means to hold it constant (statistically or
    experimentally) to isolate the causal effect of another variable

- **Example**
  - Does exercise ($X$) cause weight loss ($Y$)?
  - Confounder: Diet ($Z$) affects both exercise and weight
  - By controlling for diet (e.g., comparing people with similar diets), you can
    estimate the effect of exercise more accurately

- In **regression analysis**
  - Include $Z$ as an additional independent variable
  - E.g., in $Y = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon$
    - $\beta_1$ measures the effect of $X$ *controlling for* $Z$
    - Coefficient $\beta_1$ = _change in $Y$_ with a one-unit change in $X_1$,
      _holding $X_2$ constant_
    - Isolates $X_1$'s unique contribution
    - Compares individuals with the same $X_2$ but different $X_1$

- In **experiments**
  - Keep $Z$ constant or randomize it

## Type of Variables in Causal AI

* Mediator Variable
::: columns
:::: {.column width=60%}
- A **mediator variable** $M$ is an intermediate variable that _transmits_ the
  causal effect from $X$ (treatment) to $Y$ (outcome)
  - Lies **on the causal path** between $X$ and $Y$
  - Captures the **mechanism or process** through which $X$ influences $Y$
::::
:::: {.column width=35%}
```graphviz
digraph CausalFlow {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    X [fillcolor="#A6C8F4", label="Treatment (X)"];
    M [fillcolor="#F4A6A6", label="Mediator (M)"];
    Y [fillcolor="#A6C8F4", label="Outcome (Y)"];

    // Force ranks

    // Edges
    X -> M;
    M -> Y;
}
```
::::
:::

* Mediator Variable: Example

::: columns
:::: {.column width=65%}
- Research question: does a _training program_ increase _employee productivity_?

- The causal effect may be **indirect**, operating through a **mediator**
  - The training program might not immediately boost productivity
  - Instead, it could enhance **job satisfaction**, which in turn raises
    productivity

- **Causal interpretation**
  - $X$: Training Program (cause)
  - $M$: Job Satisfaction (mediator)
  - $Y$: Employee Productivity (effect)
  - Path: $X \rightarrow M \rightarrow Y$

- **Direct vs. Indirect effects**
  - _Indirect effect_ $X$ affects $Y$ through $M$
  - _Direct effect_ $X$ affects $Y$ not through $M$
  - Controlling for $M$ separates these two effects, clarifying *how* training
    impacts outcomes

::::
:::: {.column width=35%}

```graphviz
digraph CausalFlow {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    X [label="Training program", fillcolor="#FFD1A6", color=black];
    M [label="Job satisfaction", fillcolor="#F4A6A6", color=black];
    Y [label="Employee productivity", fillcolor="#A6E7F4", color=black];

    // Edges
    X -> M;
    M -> Y;
}
```
::::
:::

* Moderator Variable
::: columns
:::: {.column width=50%}
- A **moderator variable** changes the _strength_ or _direction_ of the
  relationship between an independent variable $(X)$ and a dependent variable
  $(Y)$
  - Moderator is not part of the causal chain but conditions the relationship
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
  // General settings
  rankdir=TB;
  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  X [label="Independent\nVariable", fillcolor="#A6C8F4", pos="0,1!"];
  Y [label="Dependent\nVariable", fillcolor="#A6C8F4", pos="2,1!"];
  M [label="Moderator\nVariable", pos="1,0!", fillcolor="#F4A6A6", style="filled,rounded,bold", color=red];
  XY [label="", shape=point, width=0.01, style=invis, pos="1,1!"];

  // Edges
  X -> XY [arrowhead=none];
  XY -> Y;
  M -> XY;

  // Use neato for fixed positioning
  layout=neato;
}
```
::::
:::

* Moderator Variable: Example

::: columns
:::: {.column width=50%}
- Research question: study relationship between stress $X$ and job performance
  $Y$

- Social support $M$ as a moderator
  - High social support weakens stress's negative effect on performance
  - Low social support strengthens stress's negative effect on performance
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
  // General settings
  rankdir=TB;
  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  X [label="Stress", fillcolor="#FFD1A6", pos="0,1!"];
  Y [label="Job\nPerformance", fillcolor="#A6E7F4", pos="2,1!"];
  M [label="Social Support", pos="1,0!", fillcolor="#F4A6A6", style="filled,rounded,bold", color=red];
  XY [label="", shape=circle, width=0.01, style=invis, pos="1,1!"];

  // Edges
  X -> XY [arrowhead=none];
  XY -> Y;
  M -> XY;

  // Use neato for fixed positioning
  layout=neato;
}
```
::::
:::


* Confounder Variable
::: columns
:::: {.column width=70%}

- A **confounder**
  - Influences multiple variables in a causal graph
  - Affects both treatment (cause) and outcome
  - Creates misleading association if not controlled

::::
:::: {.column width=30%}
```graphviz
digraph CausalTriangle {
  rankdir=TB;
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  Confounder [label="Confounder", fillcolor="#F4A6A6", color="#E57373", fontcolor="white", penwidth=2.5];
  Treatment  [label="Treatment", fillcolor="#E6E6FF"];
  Outcome    [label="Outcome", fillcolor="#E6E6FF"];

  // Edges
  Confounder -> Treatment;
  Confounder -> Outcome;
  Treatment  -> Outcome;

  // Align.
  {rank=same; Treatment; Outcome;}
}
```
::::
:::

* Confounder Variable: Example

::: columns
:::: {.column width=50%}
- $IceCreamSales$ and $Drowning$ move together
  - Correlation-based model claims association, but how to use this
    relationship?
  - Ban ice cream to prevent drowning?
  - Ice cream maker increase drowning to boost sales?

::::
:::: {.column width=30%}
```tikz
\begin{axis}[
    xlabel={Ice cream sales},
    ylabel={Drownings},
    width=10cm,
    height=6cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    title={Observed association}
]
% rising trend due to common cause "Summer"
\addplot+[only marks, mark=*, mark size=1.8pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.7) (1.2,1.1) (1.6,1.6)
    (2.1,2.4) (2.8,3.0) (3.4,3.6) (4.2,4.7) (5.1,6.0) (6.0,7.8)
};
% visual cue of spurious linear fit
%\addplot[samples=2,domain=0.2:6.2, very thick, color=red!60, dashed] {1.25*x-0.9};
\end{axis}
```
::::
:::: {.column width=20%}

```graphviz
digraph CausalTriangle {
    rankdir=TB;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    IceCreamSales [label="Eating Ice Cream", fillcolor="#A6C8F4"];
    Drowning [label="Drowning", fillcolor="#F4A6A6"];

    // Force ranks
    {rank=same; IceCreamSales; Drowning;}

    // Edges
    IceCreamSales -> Drowning;
    Drowning -> IceCreamSales;
}
```
::::
:::

\vspace{0.5cm}

::: columns
:::: {.column width=50%}
- In reality, no cause-effect between $IceCreamSales$ and $Drowning$
  - $Temperature$ is a confounder
  - When controlling for season in regression or intervention, association
    disappears

::::
:::: {.column width=30%}

```tikz
\begin{axis}[
    xlabel={Ice cream sales},
    ylabel={Drownings},
    width=10cm,
    height=6cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    title={After controlling for temperature}
]
% group 1: low temperature days
\addplot+[only marks, mark=*, mark size=1.8pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.3) (1.2,0.2) (1.6,0.4)
};
% group 2: medium temperature days
\addplot+[only marks, mark=triangle*, mark size=2pt, color=orange!70] coordinates {
    (2.1,1.1) (2.8,1.2) (3.4,1.0) (3.9,1.1)
};
% group 3: hot days
\addplot+[only marks, mark=square*, mark size=2pt, color=red!70] coordinates {
    (4.2,2.1) (5.1,2.0) (6.0,2.2) (6.5,2.1)
};
\node[anchor=south east, font=\small, text=black!70] at (rel axis cs:1.0,0.05)
{No trend within each temperature band};
\end{axis}
```

```tikz
\begin{axis}[
    xlabel={Ice Cream Sales},
    ylabel={Drownings},
    width=11cm,
    height=6.5cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    ymin=0,
    ymax=2.5,
    title={No relationship once temperature is controlled}
]
% Cool days (low temperature)
\addplot+[only marks, mark=*, mark size=2pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.3) (1.2,0.2) (1.6,0.4) (2.1,0.3) (2.8,0.2) (3.4,0.4) (3.9,0.5) (4.2,0.4) (5.1,0.3) (6.0,0.2) (6.5,0.2)
};
\node[anchor=south east, font=\small, text=black!70] at (rel axis cs:0.95,0.08)
{Flat: no causal effect};
\end{axis}
```

::::
:::: {.column width=20%}
```graphviz
digraph CausalTriangle {
    rankdir=TB;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    Temperature [label="Hot Weather", fillcolor="#FFD1A6"];
    IceCreamSales [label="Eating Ice Cream", fillcolor="#A6C8F4"];
    Drowning [label="Drowning", fillcolor="#F4A6A6"];

    // Force ranks
    {rank=same; IceCreamSales; Drowning;}

    // Edges
    Temperature -> IceCreamSales;
    Temperature -> Drowning;
}
```

::::
:::

* Collider
::: columns
:::: {.column width=60%}
- A **collider** is a variable $A$ influenced by multiple variables
  - In a causal graph $A$ with incoming edges from variables $B, C$
- A collider complicates understanding relationships between variables $B, C$
  and those it influences, $X$
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  B [label="B (Cause 1)", fillcolor="#E6E6FF"];
  C [label="C (Cause 2)", fillcolor="#E6E6FF"];
  A [label="A (Collider)", fillcolor="#F4A6A6", color="#E57373", fontcolor="white", penwidth=2.5];
  X [label="X (Effect)", fillcolor="#E6E6FF"];

  // Edges
  B -> A;
  C -> A;
  A -> X;

  // Align B and C on the same level
  {rank=same; B; C;}
}
```
::::
:::

* Collider: Examples
::: columns
:::: {.column width=60%}
- Study the relationship between $Exercise$ and $Heart Disease$
  - $Diet$ and $Exercise$ influence $Body Weight$
  - $Body Weight$ influences $Heart Disease$
  - $Body Weight$ is a collider
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise", fillcolor="#E6E6FF", fontcolor=black];
  D [label="Diet", fillcolor="#E6E6FF", fontcolor=black];
  W [label="Body Weight", fillcolor="#FFCCCC", color=red, fontcolor=black, style="filled,rounded,bold"];
  H [label="Heart Disease", fillcolor="#E6E6FF", fontcolor=black];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

* Collider Bias
- Aka "Berkson's paradox"

- Conditioning on a collider can introduce a spurious association between its
  parents by _"opening a path that is blocked"_

::: columns
:::: {.column width=70%}
- Consider the variables:
  - $Diet$ (D)
  - $Exercise$ (E)
  - $BodyWeight$ (W)
  - $HeartDisease$ (D)

- **Without conditioning on $W$**
  - $E$ and $D$ are independent
    - E.g., knowing someone's exercise level $E$ doesn't give information about
      diet $D$, and vice versa
  - The collider $W$ blocks any association between $E$ and $D$

- **After conditioning on $W$**
  - E.g., looking for individuals with specific body weight
  - You introduce a dependency between $E$ and $D$
  - Since $W$ is fixed, any change in $E$ must be balanced by a change in $D$ to
    maintain the same body weight, inducing a spurious correlation between $E$
    and $D$
::::
:::: {.column width=25%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise (E)", fillcolor="#E6E6FF", fontcolor=black];
  D [label="Diet (D)", fillcolor="#E6E6FF", fontcolor=black];
  W [label="Body Weight (W)", fillcolor="#FFCCCC", color=red, fontcolor=black, style="filled,rounded,bold"];
  H [label="Heart Disease (H)", fillcolor="#E6E6FF", fontcolor=black];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

// TODO: Add an example in the tutorial

## Paths

* Fork Structure
::: columns
:::: {.column width=70%}
- A **fork** occurs when a single variable causally influences two or more
  variables
  - Formally: $X \rightarrow C$ and $X \rightarrow D$
- $X$ is a common cause (confounder) of $C$ and $D$
- Forks induce statistical dependence between $C$ and $D$
  - Even if $C$ and $D$ are not causally linked
- Conditioning on $X$ blocks the path and removes spurious correlation
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  X
  C
  D

  X -> C;
  X -> D;

  {rank=same; C; D;}
}
```
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=70%}
- Example:
  - Lifestyle factors as confounders
  - $Lifestyle$ affects both $Weight$ and $BloodPressure$
  - These outcomes may appear correlated due to shared cause
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Lifestyle [label="Lifestyle"];
  Weight [label="Weight"];
  BP [label="Blood\nPressure"];

  Lifestyle -> Weight;
  Lifestyle -> BP;

  {rank=same; Weight; BP;}
}
```
::::
:::

* Inverted Fork

::: columns
:::: {.column width=70%}
- An **inverted fork** occurs when two or more arrows converge on a common node
  - Also known as a **collider**
- Colliders block associations unless the collider or its descendants are
  conditioned on
- Conditioning on a collider "opens" a path, inducing spurious correlations
  - This is the basis of selection bias
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph ColliderExample {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  X [label="X"];
  Y [label="Y"];
  Z [label="Z"];

  X -> Z;
  Y -> Z;

  {rank=same; X; Y;}
}
```
::::
:::

::: columns
:::: {.column width=70%}
- Example:
  - Sales influenced by multiple independent causes
  - $MarketingSpend$ and $ProductQuality$ both influence $Sales$
  - Conditioning on $Sales$ can induce false dependence between $MarketingSpend$
    and $ProductQuality$
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph ColliderExample {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Marketing [label="Marketing Spend"];
  Quality [label="Product Quality"];
  Sales [label="Sales"];

  Marketing -> Sales;
  Quality -> Sales;

  {rank=same; Marketing; Quality;}
}
```
::::
:::

* Path connecting unobserved variables
- **Unobserved variables** affect the model but we don't have a direct measure of
  it

::: columns
:::: {.column width=60%}
- E.g., consider the causal DAG
  - A retailer does market research, expecting $Price$ to influence $Sales$ in a
    predictable way
  - A retailer sets the $Price$ of a new product based on market research
  - The retailer can observe and measure $Behavior$, e.g.,
    - Discounts
    - Promotional campaign
  - There are unobserved vars that influence the model, e.g.,
    - Social media buzz
    - Word-of-mouth recommendation
::::
:::: {.column width=35%}
```graphviz
digraph CausalDAG {
  layout=neato;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Node positions
  U [label="Unobserved\nVariables", pos="2,1!"];
  M [label="Price", pos="1,1!"];
  X [label="Behavior", pos="0,0!"];
  Y [label="Sales", pos="2,0!"];

  M -> X;
  M -> Y;
  X -> Y;
  U -> Y [style=dashed];

  // Prevent layout engine from moving nodes
  edge [splines=true];
}
```
::::
:::

* Front-door Paths in Causal Inference
- A front-door path reveals causal influence through an observable mediator
  - The causal effect flows: $A \rightarrow P \rightarrow S$
- Requirements for identifiability:
  - All confounders of $A \rightarrow P$ and $P \rightarrow S$ are observed and
    controlled
  - There are no back-door paths from $A$ to $S$ through unobserved variables
- Enables causal estimation when back-door adjustment is infeasible
- Example:
  - Advertising impacts sales through customer perception of price
  - $A$: Advertising, $P$: Price perception, $S$: Sales
- Pearl's front-door criterion provides a formal method for adjustment
  - Estimate $P(P|A)$, $P(S|P,A)$, and $P(A)$ from data to compute causal effect

```graphviz[width=50%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style=rounded];

  A [label="Advertising"];
  P [label="Price"];
  S [label="Sales"];

  A -> P;
  P -> S;
  A -> S [style=dotted, label="confounding path"];

  {rank=same; P; S;}
}
```

// RESUME

* Back-Door Paths
- A company wants to understand the causal effect of price on sales

  ```graphviz[width=50%]
  digraph CausalDAG {
    rankdir=TB;
    node [shape=box, style=rounded];

    Price [label="Price"];
    Sales [label="Sales"];
    AdvSpend [label="Advertisement Spend"];

    Price -> Sales;
    AdvSpend -> Price;
    AdvSpend -> Sales;

    {rank=same; Price; Sales;}
  }
  ```

- `Price` $\rightarrow$ `Sales` is the front-door path

- A confounder is `Advertising spend` since it can affect both:
  - The price the company can set (e.g., the cost increases to cover
    advertisement costs and the product is perceived as more valuable)
  - The sales (directly)

- The back-door path goes from `Sales` to `Price` via `Advertising spend`

- The company needs to control for `Advertising spend` to estimate the causal
  effect of `Price` on `Sales` by:
  - Using `Advertising spend` as covariate in the regression
  - Designing experiment holding `Advertising spend` constant or randomized

* Frontdoor and Backdoor Paths

- Question: _Will increasing our customer satisfaction increase our sales?_

::: columns
:::: {.column width=45%}
- Assume that the Causal DAG is
  ```graphviz
  digraph CausalGraph {
    rankdir=TB;
    bgcolor="transparent";

    node [shape=box, style="rounded,filled", fontname="Helvetica"];

    // Define nodes
    PQ [label="Product Quality", fillcolor="#ffcccc", color="#ff0000", penwidth=2];
    CS [label="Cust Satisfaction", fillcolor="white"];
    S [label="Sales", fillcolor="white"];

    // Force PQ on top by putting CS and S in the same rank
    { rank = same; CS; S }

    // Define edges
    PQ -> CS;
    PQ -> S;
    CS -> S;
  }
  ```
- **Front-door path** (i.e., a direct causal relationship):
  $CustomerSatisfaction \rightarrow Sales$
::::
:::: {.column width=45%}
- **Backdoor path**: $ProductQuality$ is a common cause (confounder) of
  both $CustomerSatisfaction$ and $Sales$
- To analyze the relationship between customer satisfaction and sales, we need
  to:
  - Control for $ProductQuality$ to close the backdoor path
  - Eliminate the confounding effect
- In reality there are more confounding effects (e.g., price)
::::
:::


* Building a DAG
- **Causal models** visually represent complex environments and relationships
- Nodes are like "nouns" in the model:
  - E.g., "price", "sales", "revenue", "birth weight", "gestation period"
  - Variables can be endogenous/exogenous and observed/unobserved
  - Complex relationships between variables:
    - Parents, children (direct relationships)
    - Descendants, ancestors (along the path)
    - Neighbors
- **Iterative Refinement**:
  - Models are continuously updated with new variables and insights
- **Modeling as a Communication Tool**:
  - A shared language that bridges gaps between technical and non-technical team
    members
- **Unobservable Variables**:
  - Supports inclusion of variables not empirically observed but known to exist
  - E.g., trust or competitor activity can be modeled despite lack of direct
    data

* Heart Attack: Example
- What's the relationship between stress and heart attacks?
  - Stress is the treatment
  - Heart attack is the outcome
  - Stress is not a direct cause of heart attack
    - E.g., a stressed person tend to have poor eating habits

  ```graphviz
  digraph CausalGraph {
    rankdir=LR;
    node [shape=box, style=rounded, fontname="Helvetica", fontsize=12];

    Stress -> Diet;
    Stress -> Exercise;
    Diet -> Heart_Attacks;
    Exercise -> Heart_Attacks;
    Genetic -> Heart_Attacks;
  }
  ```

* Weights
- Weights can be assigned to paths to represent the strength of the causal
  relationship
  - Weights can be estimated using statistical methods
- Sign represents the direction

  ```graphviz
  digraph RiskFactors {
    rankdir=LR;
    node [shape=box, style=rounded, fontname="Helvetica", fontsize=12];

    Age -> Heart_Disease [label="+0.8"];
    Gender -> Heart_Disease [label="+0.3"];
    Blood_Pressure -> Heart_Disease [label="+0.7"];
    Cholesterol -> Heart_Disease [label="+0.5"];
    Exercise -> Heart_Disease [label="-0.6"];
  }
  ```

* Counterfactuals

- A **counterfactual** describes what would have happened under a different
  scenario
  - _"What would the outcome have been _if_ X had been different?"_
  - _"If kangaroos had no tails, they would topple over"_
  - _"What if we had two suppliers of our product, rather than one? Would we
    have more sales?"_
  - _"Would customers be more satisfied if we could ship products in one week,
    rather than three weeks?"_

- **Causal reasoning**:
  - Goes beyond correlation and association
  - Requires a causal model (like an SCM) to simulate alternate realities
  - E.g.,
    - Actual: A student received tutoring and scored 85%
    - Counterfactual: What if the student didn't receive tutoring?
    - Causal model estimates the alternative outcome (e.g., 70%)

- **Challenges**:
  - Requires strong assumptions and accurate models
  - Difficult to validate directly since counterfactuals are unobservable
