::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Probabilistic Programming}}$$**
\endgroup
\vspace{1cm}

::: columns
:::: {.column width=75%}
\vspace{1cm}
**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- AIMA (Artificial Intelligence: a Modern Approach)
  - Chap 15: Probabilistic programming

- Martin, Bayesian Analysis with Python, 2018 (2e)

::::
:::: {.column width=20%}

![](msml610/lectures_source/figures/book_covers/Book_cover_AIMA.jpg){ height=20% }

::::
:::

// 4, Modeling with lines

# Generalized Linear Models

## ##############################################################################
## Simple Linear Model
## ##############################################################################

* Linear Model
- Many problems can be formulated as **linear regression**:
  - $X$ and $Y$: uni-dimensional continuous RVs
  - Dataset of paired observations $\{(x_1, y_1), ..., (x_n, y_n)\}$
  - $X$ are independent variables
  - Model/predict dependent variable $Y$
  - Assume a linear relationship between $Y$ and $X$

::: columns
:::: {.column width=70%}

- E.g.,
  - Nobel laureates in a country vs amount of chocolate consumed
  - Sugar intake vs Blood glucose
  - Years of education vs Annual income
  - Advertising spend vs Sales
  - Study hours vs Exam score
  - Coffee consumption vs Productivity

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson07_Immigrant_Nobel_Prize_and_chocolate.png)

::::
:::

* Linear Model: Frequentist Approach
- A **linear model** is described by:
  $$
  y = \alpha + \beta x
  $$

- **Frequentist approach**:
  - Find parameters $\alpha, \beta$ using least square fitting
    - Minimize average quadratic error between observed $y$ and predicted
      $\hat{y}$
      $$
      \text{MSE} = \frac{1}{N} \sum_{i=1}^N (\alpha + \beta x_i - \hat{y}_i)^2
      $$
  - Point-estimate from least squares corresponds to maximum a-posteriori with
    flat (uniform) priors on $\alpha$ and $\beta$

  - With certain assumptions, it's "optimal"
    - Residuals are Gaussian
    - Linearity in parameters
    - Independence
    - Homoscedasticity (constant variance of $\varepsilon_i$)

* Linear Model: Bayesian Approach
- A **linear model** is described by the same model:
  $$
  y = \alpha + \beta x
  $$
- **Bayesian assumptions**:
  - Data $y$ is Gaussian with mean $\alpha + \beta x$ and standard deviation
    $$
    \sigma: y \sim N(\mu=\alpha + \beta x, \sigma)
    $$
  - $\alpha, \beta, \sigma$ are independent
  - Priors:
    \begin{equation*}
      \begin{cases}
      \alpha \sim Normal(\mu_\alpha, \sigma_\alpha) \\
      \beta \sim Normal(\mu_\beta, \sigma_\beta) \\
      \sigma \sim HalfNormal(0, \sigma_\epsilon) \\
      \end{cases}
    \end{equation*}

- You can use **vague priors** to add problem knowledge:
  - Prior of intercept $\alpha$ often centered around $0$
  - Info on sign of $\beta$ sometimes available
  - Half-Cauchy / exponential distribution for $\sigma$
  - Uniform prior if parameter has hard boundaries

// ## Linear model: synthetic example

* Linear Model: Synthetic Example
::: columns
:::: {.column width=35%}
- Generate random data from ground truth
  - $\alpha = 2.5$
  - $\beta = 0.9$
  - Add noise
::::
:::: {.column width=60%}
![](msml610/lectures_source/figures/Lesson07_Linear_regression_synthetic_example1.png)
::::
:::

* Linear Model: Synthetic Example
::: columns
:::: {.column width=40%}
- Create linear model in PyMC
- Use vague priors
  - Prior of intercept $\alpha$ and $\beta$ centered around $0$
  - Half-Cauchy distribution for $\sigma$

::::
:::: {.column width=55%}
![](msml610/lectures_source/figures/Lesson07_Linear_regression_synthetic_example2.png)
::::
:::

* Linear Model: Synthetic Example
::: columns
:::: {.column width=40%}
- Run the sampler
- Ground truth
  - $\alpha = 2.5$
  - $\beta = 0.9$
- Recovered values
  - $\alpha = 2.12$
  - $\beta = 0.95$

::::
:::: {.column width=55%}
![](msml610/lectures_source/figures/Lesson07_Linear_regression_synthetic_example3.png)
::::
:::

// ## Linear model: bike rental example

* Linear Model: Bike Rental Example

- Model relationship between temperature $X$ and number of bikes rented $Y$

![](msml610/lectures_source/figures/Lesson07_Bike_rental_data.png){ width=80% }

- Use intermediate variable $\mu = \alpha + \beta X$, mean number of bikes
  rented for temperature $X$
  $$
  Y \sim N(\mu, \sigma)
  $$

* Linear Model: Bike Rental Example

::: columns
:::: {.column width=45%}

- **Fit model with PyMC**: $\mu = 69 + 7.9 X$

- **Interpreting the model**
  - For temperature 0 expected rented bikes = 69
  - Each degree increase: expected value increases by 7.9

- **Parameters have uncertainty**
  - Posterior accounts for combined uncertainty
  - Bands represent quantiles [0.25, 0.75] and [0.03, 0.97] of prediction

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson07_Bike_rental_data_2.png)
![](msml610/lectures_source/figures/Lesson07_Bike_rental_data_3.png)

::::
:::

- **Do you see any problem in the model?** ![](emoji/puzzle.png){ width=14px }

* Linear Model: Bike Rental Example (Criticism)

- **Problems**
  1. Model outputs negative bike numbers
  2. Model predicts real numbers, but count is discrete

- **Root cause**: Not surprising since using Gaussian likelihood:
  - Gaussian extends to negative numbers
  - Gaussian is continuous

- **Solutions**:
  - Hack: clip predictions below 0 and discretize output
  - Elegant: use model defined only for discrete positive numbers

* Count Data
- **Count data** = when the RV is a discrete number and bounded at 0
  - E.g., the number of rented bikes
- If the number is large, we can use a continuous distribution
  - E.g., Gaussian
- Other times, it's modeled as discrete
  - E.g., Poisson, negative binomial

* Generalized Liner Model (GLM)
- **Generalization of linear model** using different distributions for
  likelihood:
  \begin{equation*}
    \begin{cases}
    \alpha \sim prior \\
    \beta \sim prior \\
    \mu = \alpha + \beta X \\
    \theta \sim prior \\
    Y \sim \phi(f(\mu), \theta) \\
    \end{cases}
  \end{equation*}
  where:
  - $\phi$: arbitrary distribution
    - E.g., Normal, Student's t, negative binomial
  - $\theta$: auxiliary parameter for $\phi$
    - E.g., $\sigma$ for Normal
  - $f()$: "inverse link function" transforming $\mu$ from the real line to
   domain of $\phi$

// ## 4.4 Counting bikes

* Poisson Distribution

::: columns
:::: {.column width=50%}
- **Poisson distribution**
  - Models number of events in a fixed interval (e.g., time, space)
  - Assumes events happen independently at a constant average rate

- **E.g.,**
  - Call centers: number of customers per hour
  - Natural events: number of earthquakes in a region over time
  - Traffic flow: number of cars through a toll booth per day
  - Biology: count mutations in a DNA segment over time

- **Cons**:
  - Assumes mean equals variance (can't model "overdispersion")
::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson07_Poisson_PMF.png)

::::
:::

* Overdispersion
- **Overdispersion** occurs when variance of count data is much larger (e.g.,
  15-20x) than mean

- **Examples**:
  - _Epidemiology_: modeling disease outbreak, number of new infections
    fluctuates widely
  - _Customer service_: large variation in daily complaints

- **How to model it?**
  - **Model using Poisson**:
    - Mean and variance needs to be equal ![](emoji/wrong.png){ width=12px }
  - **Model using Normal**:
    - Mismatch predicting negative rented bikes
    - Poor fit on positive side
  - **Model using Negative Binomial**:
    - Better fit, though not perfect
    - Right tail predictions differ, but high demand probability is low
    - Second parameter controls variance 
    - Overall better than Normal model ![](emoji/check_mark.png){ width=12px }

* Negative Binomial Distribution
::: columns
:::: {.column width=50%}
- **Negative binomial distribution**
  - Models failures/trials to achieve fixed number of successes in IID Bernoulli
    trials
  - Generalizes geometric distribution, modeling trials to first success
  - Models overdispersion (mean $\ll$ variance)

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson07_Neg_binomial_PMF.png)

::::
:::

- **E.g.,**
  - Customer service: unsuccessful interactions before success
  - Sports: games lost before winning a fixed number of games

// ## 4.5, Robust regression

* Robust Regression
::: columns
:::: {.column width=40%}
- Outliers pull regression line away from bulk of data

- Robust regression is just another generalized linear model
  - Instead of some feature transform / winsorization like in the hacker ML

::::
:::: {.column width=55%}
![](msml610/lectures_source/figures/Lesson07_Non_robust_regression1.png)
::::
:::

- Student's t-distribution
  - Has heavier tails than Normal
  - Gives less importance to outliers

::: columns
:::: {.column width=45%}

//![](msml610/lectures_source/figures/Lesson07_Robust_regression_code.png)
![](msml610/lectures_source/figures/Lesson07_Robust_regression_model.png)

::::
:::: {.column width=45%}
![](msml610/lectures_source/figures/Lesson07_Non_robust_regression2.png)

::::
:::

// ## 4.6, Logistic regression

## #############################################################################
## Logistic Regression
## #############################################################################

* Logistic Regression
- **Logistic regression model**
  - Is a generalized linear model
  - Models the response variable as binary
    - E.g., ham/spam, safe/unsafe, cloudy/sunny, hotdog/not hotdog

- The **logistic model** is:
  \begin{equation*}
    \begin{cases}
    \theta = logit(\alpha + \beta x) \\
    y \sim Bernoulli(\theta) \\
    \end{cases}
  \end{equation*}

::: columns
:::: {.column width=40%}

- Logistic function (aka sigmoid)

  $$
  logit(z) = \frac{1}{1 + \exp^{-z}}
  $$
  - Converts real numbers from $\theta$ to [0, 1] for the Bernoulli distribution

::::
:::: {.column width=55%}

```tikz
\begin{axis}[
    width=12cm, height=7cm,
    xmin=-6, xmax=6,
    ymin=-0.05, ymax=1.05,
    axis lines=left,
    xlabel={$x$},
    ylabel={$P(y=1 \mid x)$},
    domain=-6:6,
    samples=400,
    grid=both,
    minor grid style={gray!15},
    major grid style={gray!25},
    legend style={draw=none, fill=none, at={(0.02,0.98)}, anchor=north west, font=\small},
    tick label style={font=\small},
    label style={font=\small},
  ]

  % --- logistic curve: P(y=1|x) = 1/(1 + exp(-(a x + b)))
  % adjust a (slope) and b (intercept) to taste:
  \def\a{1.2}
  \def\b{-0.5}
  \addplot[very thick] {1/(1 + exp(-(\a*x + \b)))}; 

  % --- decision threshold at 0.5
  \addplot[densely dashed] coordinates {(-6,0.5) (6,0.5)};
  \addlegendentry{Threshold $=0.5$}

  % --- implied decision boundary x* where a x + b = 0  =>  x* = -b/a
  \pgfmathsetmacro{\xb}{-(\b)/(\a)}
  \addplot[densely dotted] coordinates {(\xb, -0.05) (\xb, 1.05)};
  \addlegendentry{Boundary $x^\star=-b/a$}

  % --- optional: annotate boundary
  \node[anchor=north east, font=\small] at (axis cs:\xb,0.0) {$x^\star$};

\end{axis}
```
::::
:::

* Iris Dataset
- Classical dataset of measurements of flowers from 3 closely related species
  of Iris setosa, virginica, and versicolor
  - E.g., we want to predict the probability of a flower being `setosa` from the
    `sepal_length`

![](msml610/lectures_source/figures/Lesson07_Logistic_regression_df.png){ width=90% }

* Classification with Logistic Regression

- **Logistic regression**:
  - Computes the probability that the output is a certain value
  - Models $\theta = \Pr(Y=1 | X)$
  - Classifies the output using a decision rule, e.g.,
    $$
    \Pr(Y = \texttt{versicolor} | \texttt{sepal\_length}) > 0.5
    $$

- Same treatment as frequentist approach, since it has a Bayesian flavor
  - But once it's Bayesian, you can generalize even more!
  - No gradient descent, but solved with Bayesian inference

- **Classification with logistic regression** might seem a **misnomer**
  - **Regression** predicts a continuous variable given input variables
  - **Classification** predicts a discrete variable given input variables
  - Is a "regression" since it computes the probability that the output is a
    certain value

* Boundary Decision for a Classifier
- **Boundary Decision** $\delta$ for a classifier = values of independent
  variables making probability equal to 0.5

- For logistic regression $\delta$:
    \begin{align*}
    & 0.5 = logit(\alpha + \beta \delta) \\
    & 0 = \alpha + \beta \delta\\
    & \delta = - \frac{\alpha}{\beta} \\
    \end{align*}
  - Decision boundary has uncertainty due to uncertainty in $\alpha$ and $\beta$

- The probability threshold $0.5$ chosen for equal misclassification risk
  - Misclassification cost may not be symmetrical
  - E.g., minimize false negatives ("patient has disease, not predicted") or
    false positives ("patient doesn't have disease, predicted")

* Odds
- The **odds of event** "$y=1$" is the ratio between favorable vs unfavorable
  events
  $$
  \text{odds} \defeq \frac{\Pr(y=1)}{1 - \Pr(y=1)}
  $$

- Transformation between probability, odds, and log-odds
  - More intuitive than probability in "gambling"
  - E.g., odds of getting a 2 rolling a fair die are
    $\frac{1/6}{5/6} = \frac{1}{5} = 0.2$
    - One favorable event for 5 unfavorable events

- Interpreting **logistic regression in terms of odds**:
  - Since $\theta = logit(\alpha + \beta x)$ and $\theta = \Pr(y = 1)$, we get:
    $$
    \alpha + \beta x = \log ( \frac{\Pr(y=1)}{1 - \Pr(y=1)} )
    $$
  - Logistic regression models log-odds as linear regression
  - $\beta$ is the increase of log-odds for a unit change of $x$

* Classification with Logistic Regression: Bayesian

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Logistic_regression_code.png)

::::
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Logistic_regression_model.png){ height=50% }

::::
:::

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Logistic_regression_result.png)

::::
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Logistic_regression_result2.png)

::::
:::

* Heteroskedasticity
- **Heteroskedasticity** = when variance of errors is not constant across
  observations
  - E.g., variance can be a linear function of the dependent variable

- **Example**:
  - Baby height as a function of age is heteroskedastic
  - $\sigma$ is a linear function of the predictor variable
  - Mean $\mu$ is square root of a linear model

![](msml610/lectures_source/figures/Lesson07_Variable_variance_data.png)

* Heteroskedasticity: Bayesian Model
::: columns
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson07_Variable_variance_code.png)

::::
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Variable_variance_model.png)

::::
:::

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Variable_variance_result.png)

::::
:::: {.column width=50%}

::::
:::

//## Hierarchical linear regression
//
//// TODO: Finish
//
//* Hierarchical linear regression
//- Hierarchical models are a powerful concept that allows us to model complex data
//  structures
//- We can do inference at / above the group level at the same time
//- Groups can share information by using hyper-priors, which provides shrinkage
//  and regularizes estimates
//
//- E.g., hierarchical linear regression models
//
//* Divergences after tuning
//- Sometimes PyMC reports "divergences after tuning" (e.g., this is the case with
//  linear models)
//  - Samples generated by PyMC may not be trustworthy
//
//- Solutions
//  1) You can increase `target_accept` in `pm.sample()`
//  2) Re-parametrize model = re-write model in a different way to help the sampler
//     (e.g., remove the divergences) or the model's interpretability
//
//* Centered vs non-centered hierarchical models
//- Hierarchical centered
//  - We estimate the parameters for the individual groups
//- Hierarchical non-centered
//  - Estimate common parameters for all groups and then the deflection for each
//    group
//  - The information is represented in the same way

## Multiple Linear Regression

* Multiple Linear Regression
- In prediction problems, it is common to have several independent variables
  - E.g., student's grades = f(family income, mother's education, ...)

- **Problem formulation**
  - $k$ independent variables
  - $N$ observations
  - Find a hyperplane of dimension $k$ to explain data
  $$
  \mu = \alpha + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k
  $$
  - Same as polynomial regressions but with independent variables

* Multiple Regression: Synthetic Example 1/2

- Generate random data

::: columns
:::: {.column width=40%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression1.png)
::::
:::: {.column width=55%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression2.png)
::::
:::

- Create PyMC model

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model_code.png)
::::
:::: {.column width=45%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model.png)
::::
:::

* Multiple Regression: Synthetic Example 2/2

- Solve model

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_results1.png)
::::
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_results2.png)
::::
:::

* Multiple Regression: Rented Bike Example 1/2

- **Assumption**: number of bike rented is function of temperature and hour of
  the day

- Create PyMC model

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model_RentedBikes_model_code.png)
::::
:::: {.column width=45%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model_RentedBikes_model.png)
::::
:::

* Multiple Regression: Rented Bike Example 2/2

- Solve model

::: columns
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model_RentedBikes_model_trace.png)

::::
:::: {.column width=50%}
![](msml610/lectures_source/figures/Lesson07_Multiple_linear_regression_model_RentedBikes_model_results.png)
::::
:::

* Tutorial

- [Generalized Linear Models](https://github.com/gpsaggese/umd_classes/blob/master/msml610/tutorials/notebooks/Lesson07.04_Generalized_Linear_Models.ipynb)
