::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{8.2: Causal Inference}}$$**
\endgroup

\vspace{1cm}

::: columns
:::: {.column width=75%}
**Instructor**: Dr. GP Saggese - [](gsaggese@umd.edu)

**References**:

- AIMA (Artificial Intelligence: a Modern Approach)

- Pearl et al., The Book of Why, 2017
::::
:::: {.column width=20%}

![](msml610/lectures_source/figures/book_covers/Book_cover_AIMA.jpg){width=1.5cm}

![](msml610/lectures_source/figures/book_covers/Book_cover_Book_of_why.jpg){width=1.5cm}

::::
:::

# Causal Networks

// ## 13.5 Causal networks (p. 462)

* (Non-Causal) Bayesian Networks
- **Bayesian networks** represent a joint distribution function
  - The direction of the arrow represent **conditional dependence** (not
    causality)
    - $A \to B$ requires to estimate $\Pr(A | B)$

::: columns
:::: {.column width=60%}

- There are **many possible** _edges_ and _node ordering_ for the same Bayesian
  network
- E.g., a Bayesian network with $Fire$ and $Smoke$, which are dependent
  - $Fire \to Smoke$
    - Need $\Pr(Fire)$ and $\Pr(Smoke | Fire)$ to compute $\Pr(Fire, Smoke)$
  - $Smoke \to Fire$
  - Need $\Pr(Smoke)$ and $\Pr(Fire | Smoke)$
::::
:::: {.column width=35%}
```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Fire -> Smoke [label="P(Smoke | Fire)"];
}
```

\vspace{1cm}

```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Smoke -> Fire [label="P(Fire | Smoke)"];
}
```
::::
:::

- Different Bayesian networks:
  - Are equivalent and convey the same information
  - Have different difficulties to be estimated

- At the same time, there is an **asymmetry in nature**
  - Extinguishing fire stops smoke
  - Clearing smoke doesn't affect fire

* Causal (Bayesian) Networks
- **Causal networks** are Bayesian networks forbidding **non-causal edges**

- Use judgment based on nature instead of just statistics
  - E.g., from _"Are random variables $Smoke$ and $Fire$ correlated?"_ to _"What
    causes what, $Smoke$ or $Fire$?"_

- **"Dependency in nature"** is like assignment in programming
  - E.g., nature assigns $Smoke$ based on $Fire$:
    - ![](emoji/check_mark.png){ width=12px } $Smoke := f(Fire)$
    - ![](emoji/wrong.png){ width=12px } $Fire := f(Smoke)$

- **Structural equations** describe "assignment mechanism" in causal graphs
  $$
  X_i := f(X_j) \iff X_j \to X_i
  $$

* Causal DAG
- **Causal DAG**
  - _Directed_: Arrows show cause $\rightarrow$ effect
  - _Acyclic_: No feedback loops
    - Causal relationships assume temporal order: cause before effect
    - A cycle implies a variable is both cause and effect of itself

- **Benefits**
  - DAGs encode _causal_ links
  - Reason about interventions and counterfactuals
  - Support explainable AI models
  - Stability in conditional probability estimation

- **Limitations**
  - Requires domain knowledge for structure
  - Assumes all relevant variables included (no hidden confounders)

* Causal Edges are Stable

- **Causal edge** $X \rightarrow Y$ shows direct causal influence of $X$ on $Y$,
  holding other variables constant
  - Captures how manipulating $X$ changes $Y$, not just their covariance

- Causal edges reflect **stable relationship**

  - Mechanistic stability
    - Causal relationships show system function, not just behavior in one dataset
    - E.g., "Temperature $\rightarrow$ ice melting rate" holds true in Alaska and
      Arizona

  - Invariance under interventions
    - If $X$ causes $Y$, intervening on $X$ affects $Y$ consistently, despite
      confounders or context changes

  - Easier estimation through causal modeling
    - Identifying causal direction focuses estimation on effect size (e.g.,
      regression of $Y$ on $X$ under intervention)

  - Reduced sensitivity to sampling and omitted variables
    - Correlations may change with confounder addition or removal
    - True causal edge persists, stable across model specifications

- **Example**: study $Exercise \rightarrow Health$:
  - Correlation may differ in young or elderly populations
  - Causal effect remains stable, as physiological mechanism doesn't change

* Structural Causal Model
- A **Structural Causal Model** (SCM) translates a causal DAG into mathematical
  equations
  - DAGs show structure (variables and arrows)
  - SCMs use equations to define how variables interact

- **Structure of SCMs**
  - Variables $X_1, X_2, ..., X_n$ represent quantities in the system
  - Equations model each variable as a function of its direct causes
  - Formally, $X_i$ is modeled as:
    $$X_i = f_i(Parents(X_i), \varepsilon_i)$$
    where:
    - $Parents(X_i)$ are direct causes of $X_i$
    - $\varepsilon_i$ is an exogenous (external, unobserved) noise term

- **Properties**
  - Explain causal relationships between variables
  - Provide a foundation for causal reasoning and simulation
  - Describe how the world works, not just variable correlations

* Structural Causal Model: Example
::: columns
:::: {.column width=50%}
- **Explanatory variables**
  - You can manipulate or observe when changes are applied
  - E.g., _"does a large cup of coffee before an exam help with a test?"_

- **Outcome variables**
  - Result of the action
  - E.g., _"by how much did the score test improve?"_
::::
:::: {.column width=45%}
```graphviz
digraph SCMExample {
  rankdir=TB;
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  Coffee [label="Large Cup of Coffee", fillcolor="#b3cde3"];
  TestScore [label="Test Score Improvement", fillcolor="#ccebc5"];
  RoomTemp [label="Room Temperature", fillcolor="#decbe4"];

  // Edges
  Coffee -> TestScore;
  RoomTemp -> TestScore [style=dashed];
}
```
::::
:::

- **Unobserved variables**
  - Not seen or more difficult to account
  - E.g., _"temperature of the room makes students sleepy and less alert"_

* Structural Causal Model: Sprinkler Example
::: columns
:::: {.column width=25%}

```graphviz
digraph BayesianFlow {
    rankdir=TD;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    Cloudy        [label="Cloudy",        fillcolor="#b3cde3"];
    Sprinkler     [label="Sprinkler",     fillcolor="#ccebc5"];
    Rain          [label="Rain",          fillcolor="#ccebc5"];
    WetGrass      [label="Wet Grass",     fillcolor="#decbe4"];
    GreenerGrass  [label="Greener Grass", fillcolor="#fed9a6"];

    // Force ranks
    { rank=same; Sprinkler; Rain; }

    // Edges
    Cloudy -> Sprinkler;
    Cloudy -> Rain;
    Sprinkler -> WetGrass;
    Rain -> WetGrass;
    WetGrass -> GreenerGrass;
}
```
::::
:::: {.column width=70%}
- Structural equations for this model:
  \begingroup \small
  $$
  \left\{
  \begin{aligned}
    & C := f_C(\varepsilon_C) \\
    & R := f_R(C, \varepsilon_R) \\
    & S := f_S(C, \varepsilon_S) \\
    & W := f_W(R, S, \varepsilon_W) \\
    & G := f_G(W, \varepsilon_G) \\
  \end{aligned}
  \right.
  $$
  \endgroup

- Unmodeled variables $\varepsilon_x$ represent error terms
  - E.g., $\varepsilon_W$ is another source of wetness besides $Sprinkler$ and $Rain$
    (e.g., $MorningDew$)
- Assume unmodeled variables are exogenous, independent, with a certain
  distribution (prior)
::::
:::

- Express joint distribution of five variables as a product of conditional
  distributions using causal DAG topology:
  \begingroup \small
  $$
  \Pr(C, R, S, W, G) = \Pr(C) \Pr(R|C) \Pr(S|C) \Pr(W|R,S) \Pr(G|W)
  $$
  \endgroup

## #############################################################################
## Variables
## #############################################################################

* Observed Vs. Unobserved Variables
::: columns
:::: {.column width=45%}
- **Observed variables**
  - Aka "measurable" or "visible"
  - Variables directly measured or collected in a dataset
  - E.g.,
    - Education
    - Income
    - Blood pressure
    - Product price

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_ObservedUnobserved_Vertical {
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#ccebc5"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#decbe4"];
    Weather         [label="Weather",         fillcolor="#decbe4"];
    News            [label="News",            fillcolor="#decbe4"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend (top, vertical stack)
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Observed", shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key2 [label="Unobserved",  shape=box, style="rounded,filled", fillcolor="#decbe4"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::

- **Unobserved variables**
  - Aka "latent" or "hidden"
  - Exist but not measured or included in data
  - E.g.,
    - Natural talent
    - Motivation
    - Company culture
  - Ignoring unobserved variables distorts causal relationships
    - Observed: $IceCreamSales$ and $DrowningRates$
    - Unobserved: $Temperature$
    - Misleading conclusion: $IceCream$ causes $Drowning$

* Endogenous Vs. Exogenous Variables
::: columns
:::: {.column width=45%}
- **Endogenous variables**
  - Values determined _within_ the model
    - Dependent on other variables in the system
  - Represent system's internal behavior and outcomes
  - E.g.,
    - Motivation
    - Income

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#fed9a6"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#fed9a6"];
    Motivation      [label="Motivation",      fillcolor="#b3cde3"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend at top
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Endogenous", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::
- **Exogenous variables**
  - Originate _outside_ the system being modeled
    - Not caused by other variables in the model
  - Represent background conditions or external shocks
  - E.g.,
    - Natural talent
    - Economic policy
    - Weather
    - News

* Endo / Exogenous, Observed / Unobserved Vars
::: columns
:::: {.column width=40%}
- In **Structural Causal Models**
  $$
  X_i = f_i(Parents(X_i), \varepsilon_i)
  $$
  where:
  - $X_i$: endogenous
  - $\varepsilon_i$: exogenous noise

- **Typically**
  - _Endogenous variables_: focus for prediction and intervention
  - _Exogenous variables_: capture randomness or unknown external factors

::::
:::: {.column width=55%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical layout for legend

        key1 [label="Endogenous / Observed", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous / Observed",  shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key3 [label="Endogenous / Unobserved", shape=box, style="rounded,filled", fillcolor="#decbe4"];
        key4 [label="Exogenous / Unobserved",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
        key2 -> key3 [style=invis];
        key3 -> key4 [style=invis];
    }
}
```

\begingroup \scriptsize
| **Variable Type**        | **Observability**   | **Example**          |
|---------------------------|---------------------|---------------------|
| Endogenous               | Observed            | Income               |
| Exogenous                | Observed            | Education            |
| Endogenous               | Unobserved          | Motivation           |
| Exogenous                | Unobserved          | Natural Talent       |
\endgroup

::::
:::

* Building a Causal DAG
- **Causal models** visually represent complex environments and relationships

- Nodes are like "nouns" in the model:
  - E.g., "price", "sales", "revenue", "birth weight", "gestation period"
  - Variables can be endogenous/exogenous and observed/unobserved
  - Complex relationships between variables:
    - Parents, children (direct relationships)
    - Descendants, ancestors (along the path)
    - Neighbors

- **Iterative Refinement**:
  - Continuously update models with new variables and insights

- **Modeling as a Communication Tool**:
  - Shared language bridges gaps between technical and non-technical team
    members

- **Unobservable Variables**:
  - Include variables not empirically observed but known to exist
  - E.g., trust or competitor activity modeled despite lack of direct data

* Heart Attack: Example
::: columns
:::: {.column width=50%}
- What's the relationship between stress and heart attacks?
  - Stress is the treatment
  - Heart attack is the outcome
  - Stress is not a direct cause of heart attack
    - E.g., a stressed person tend to have poor eating habits
::::
:::: {.column width=45%}

```graphviz
digraph CausalGraph {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Stress [fillcolor="#F4A6A6"];
    Diet [fillcolor="#FFD1A6"];
    Exercise [fillcolor="#B2E2B2"];
    Heart_Attacks [fillcolor="#A0D6D1"];
    Genetic [fillcolor="#A6E7F4"];

    Stress -> Diet;
    Stress -> Exercise;
    Diet -> Heart_Attacks;
    Exercise -> Heart_Attacks;
    Genetic -> Heart_Attacks;
}
```
::::
:::

* Weights
- Assign weights to paths to represent causal strength
  - Estimate weights using statistical methods

- Sign indicates direction

```graphviz
digraph RiskFactors {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Age [fillcolor="#F4A6A6"];
    Gender [fillcolor="#FFD1A6"];
    Blood_Pressure [fillcolor="#B2E2B2"];
    Cholesterol [fillcolor="#A0D6D1"];
    Exercise [fillcolor="#A6E7F4"];
    Heart_Disease [fillcolor="#A6C8F4"];

    Age -> Heart_Disease [label="+0.8"];
    Gender -> Heart_Disease [label="+0.3"];
    Blood_Pressure -> Heart_Disease [label="+0.7"];
    Cholesterol -> Heart_Disease [label="+0.5"];
    Exercise -> Heart_Disease [label="-0.6"];
}
```


//
//## Intervention
//
//* Estimating Causal Effects
//
//- **Goal**: Determine the causal effect of a treatment variable (aka
//  intervention) $T$ on an outcome $Y$
//
//::: columns
//:::: {.column width=60%}
//- **Example:**
//    - $T$ = _"takes drug"_
//    - $Y$ = _"recovers"_
//    - $C$ = _"overall health"_
//- Healthier people may take medicine and recover faster $\implies$ correlation
//  without causation
//
//- In **observational data**
//  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
//  - $C$ creates _spurious correlation_ between $T$ and $Y$
//::::
//:::: {.column width=35%}
//```graphviz
//digraph Observational {
//    splines=true;
//    nodesep=1.0;
//    ranksep=0.75;
//    rankdir=TD;
//
//    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//    // Node styles
//    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
//    T [label="Treatment (T)", fillcolor="#A6C8F4"];
//    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
//
//    // Edges
//    C -> T [label="affects"];
//    C -> Y [label="affects"];
//    T -> Y [label="causal effect?"];
//}
//```
//::::
//:::
//
//- **Problem**
//  - There is a "backdoor path" $Treatment \leftarrow Confounder \rightarrow
//    Outcome$
//
//* Frontdoor and Backdoor Paths: Intuition
//::: columns
//:::: {.column width=55%}
//
//- A **backdoor path** is any path from $T$ to $Y$ starting with an arrow into
//  $T$
//  - E.g., $T \leftarrow C \rightarrow Y$
//  - Interpretation:
//    - $C$ is a common cause of $T$ and $Y$, confounding their relationship
//    - Controlling (conditioning) for $C$ blocks the backdoor path, identifying
//      the causal effect of $T$ on $Y$
//::::
//:::: {.column width=40%}
//
//```graphviz
//digraph Observational {
//    splines=true;
//    nodesep=1.0;
//    ranksep=0.75;
//    rankdir=TD;
//
//    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//    // Node styles
//    C [label="Confounder (C)",fillcolor="#FFD1A6"];
//    T [label="Treatment (T)", fillcolor="#A6C8F4"];
//    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
//
//    // Edges
//    C -> T [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
//    C -> Y [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
//    T -> Y [label="Frontdoor", color="#1E8449", fontcolor="#1E8449", penwidth=2.5];
//}
//```
//
//::::
//:::
//
//- A **frontdoor path** goes directly or indirectly from $T$ to $Y$ through
//  mediators, following causal flow
//  - E.g., $T \rightarrow Y$
//  - Interpretation:
//    - Direct causal path of interest
//    - No mediators, so front-door path is direct causal effect of $T$ on $Y$
//
//* Randomized Controlled Trials (RCTs)
//
//- **Randomized Controlled Trial** is an experimental study to assess causal
//  effect of an intervention or treatment
//  - Determine whether an intervention causes an effect, not just associated with
//    it
//  - Eliminate selection bias and confounding variables through randomization
//
//- **Key Components**
//  - _Randomization_: ensures groups are statistically equivalent at baseline
//  - _Control Group_: receives a placebo or standard treatment
//  - _Blinding_: participants and/or researchers do not know the assignment to
//    avoid bias
//  - _Outcome Measurement_: pre-defined metrics assess the intervention's effect
//
//- **Example**: testing a new drug
//  - Treatment group receives the new drug
//  - Control group receives a placebo
//  - Compare recovery rates after a fixed period
//
//- **Pros**
//  - Provides clear causal inference due to randomization
//
//- **Cons**
//  - Expensive and time-consuming
//  - Ethical or practical constraints may prevent randomization
//
//* RCTs Solve the Problem of Confounders
//::: columns
//:::: {.column width=65%}
//- In **observational data**
//  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
//  - $C$ creates _spurious correlation_ between $T$ and $Y$
//::::
//:::: {.column width=30%}
//```graphviz
//digraph Observational {
//    splines=true;
//    nodesep=1.0;
//    ranksep=0.75;
//    rankdir=TD;
//
//    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//    // Node styles
//    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
//    T [label="Treatment (T)", fillcolor="#A6C8F4"];
//    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
//
//    // Edges
//    C -> T [label="affects"];
//    C -> Y [label="affects"];
//    T -> Y [label="causal effect?"];
//}
//```
//::::
//:::
//
//\vspace{0.5cm}
//
//::: columns
//:::: {.column width=55%}
//- In **experimental settings**
//  - Randomization ($R$) breaks link between $C$ and $T$
//  - Random assignment prevents influence on both treatment and outcome
//  - $T$ is independent of $C$: $T \perp C$
//  - Only open path between $T$ and $Y$ is causal path $T \rightarrow Y$
//::::
//:::: {.column width=40%}
//```graphviz
//digraph RCT {
//  splines=true;
//  nodesep=1.0;
//  ranksep=0.75;
//  rankdir=TD;
//
//  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//  // Node styles
//  R [label="Randomization", shape=diamond, fillcolor="#C6A6F4"];
//  T [label="Treatment (T)", fillcolor="#A6C8F4"];
//  Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
//  C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
//
//  // Edges
//  R -> T [label="assigns"];
//  T -> Y [label="causal effect!"];
//  C -> Y [label="affects"];
//}
//```
//::::
//:::
//
//* Causal Graphs and Interventions
//- **Observing correlations** between variables _does not reveal causality_
//  - $\Pr(Y | T)$ confounds direct and indirect influences
//
//- **Randomized Controlled Trials** provide the _gold standard_ for causal
//  inference
//  - Randomization breaks all back-door (confounding) paths
//  - RCTs are expensive, slow, or ethically impossible
//
//- **Alternative solution**
//  - Can we estimate the _causal effect_ from _observational data alone_?
//  - Under _what conditions_ and using _which variables_?
//
//- **Idea**: Identify and condition on the right _confounders_ to:
//  - Block spurious associations between $T$ and $Y$
//  - Recover the true causal effect $\Pr(Y | do(T))$
//
//* Intervention in Structural Equations
//- **Purpose of Structural Equations**
//  - Capture causal mechanisms among variables
//  - Predict impact of external interventions
//
//- **Effect of Intervention** $do(X_j = x_j)$
//  - Original equation:
//    $$
//    X_j = f_j(Parents(X_j), \varepsilon_j)
//    $$
//  - Modified by intervention:
//    $$
//    X_j = x_j \text{ (fixed value)}
//    $$
//  - "Mutilate" causal network by _removing incoming edges_ to $X_j$
//  - Recompute joint distribution of all variables using modified structure
//
//- **Intuition**
//  - $do$-operator enforces variable's value externally, breaking causal
//    dependencies
//  - Enables reasoning about "what would happen if...?" scenarios
//
//* Adjustment Formula in Causal Networks
//
//- **Goal**
//  - Estimate causal effect of intervention $do(X_j = x_{jk})$ on another variable $X_i$
//
//- **The Adjustment Formula**
//  - Derived from the post-intervention joint distribution:
//    $$
//    \Pr(X_i = x_i | do(X_j = x_{j}^*)) =
//    \sum_{Parents(X_j)}
//    \Pr(x_i | x_{j}^*, Parents(X_j))
//    \Pr(Parents(X_j))
//    $$
//  - The mechanism for $X_j$ is _removed_: it is treated as a fixed cause, not a
//    random variable
//
//- **Interpretation**
//  - Computes a _weighted average_ of effects of $X_j$ and its parents on $X_i$
//  - Weights come from prior probabilities of the parents’ values
//
//- **Back-Door Criterion**
//  - A set $Z$ is a valid adjustment set if it blocks _all back-door paths_ from
//    $X_j$ to $X_i$
//  - Ensures $X_i \perp \text{Parents}(X_j) | X_j, Z$
//
//- **Why It Matters**
//  - Enables causal inference from observational data
//  - Estimate treatment and policy effects _without randomized trials_
//
//* Backdoor Criterion: Definition
//- A set of variables $Z$ satisfies the **backdoor criterion** for variables $X$
//  (cause) and $Y$ (effect) in a causal graph if:
//  1. **No element of $Z$ is a descendant of $X$**
//     - Ensures $Z$ does not "block" part of the causal effect of $X$ on $Y$
//     - Descendants of $X$ may carry information about the causal effect and
//       should not be controlled for
//  2. **$Z$ blocks every path between $X$ and $Y$ containing an arrow into $X$**
//     - These paths are _backdoor paths_, representing potential confounding
//       influences
//     - Blocking them ensures any remaining association between $X$ and $Y$ is
//       causal, not spurious
//
//```graphviz[width=70%]
//digraph Backdoor_Criterion {
//    splines=true;
//    nodesep=1.0;
//    ranksep=0.75;
//
//    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//    // Node styles
//    X [label="X (Cause)", fillcolor="#A6C8F4"];
//    Y [label="Y (Effect)", fillcolor="#B2E2B2"];
//    Z [label="Z (Control)", fillcolor="#FFD1A6"];
//    U [label="U (Confounder)", fillcolor="#C6A6F4"];
//    DescX [label="Descendant of X", fillcolor="#F4A6A6"];
//
//    // Force ranks
//    { rank=same; X; Y; Z}
//
//    // Edges
//    X -> Y [label="Causal Path", color="#4A90E2", penwidth=2];
//    U -> X [label="Backdoor", color="#7E57C2", style=dashed];
//    U -> Y [color="#7E57C2", style=dashed];
//    Z -> X [label="Confounding", color="#FFA500"];
//    Z -> Y [color="#FFA500"];
//    X -> DescX [label=" Causal Descendant", color="#E57373"];
//}
//```
//
//* Backdoor Criterion: Intuition
//- **Intuition**:
//  - The goal is to isolate the causal effect of $X$ on $Y$ by eliminating
//    *confounding bias*
//  - Controlling for an appropriate set $Z$ makes the relationship between $X$ and
//    $Y$ as if $X$ were randomly assigned
//
//- **Application**:
//  - When $Z$ satisfies the backdoor criterion, we can estimate causal effects
//    from **observational data** (without experiments)
//  - The causal effect can be computed using:
//    $$
//    \Pr(Y | do(X)) = \sum_z \Pr(Y | X, Z=z) P(Z=z)
//    $$
//
//* Intervention: Sprinkler Example
//::: columns
//:::: {.column width=70%}
//- "Intervene" by turning the sprinkler on
//  - In do-calculus $do(Sprinkler = T)$
//  - Sprinkler variable $s$ is independent of cloudy day $c$
//
//- Structural equations after intervention:
//  \begingroup \small
//  $$
//  \left\{
//  \begin{aligned}
//  & C := f_C(\varepsilon_C) \\
//  & R := f_R(C, \varepsilon_R) \\
//  & S := True \\
//  & W := f_W(R, S, \varepsilon_W) \\
//  & G := f_G(W, \varepsilon_G) \\
//  \end{aligned}
//  \right.
//  $$
//  \endgroup
//
//::::
//:::: {.column width=25%}
//
//```graphviz
//digraph GrassBayes {
//    rankdir=TB;
//    nodesep=1.0;
//    ranksep=0.75;
//    splines=true;
//    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];
//
//    // Nodes with custom colors
//    Cloudy        [label="Cloudy",        fillcolor="#c6dbef"];
//    Rain          [label="Rain",          fillcolor="#c6dbef"];
//    Sprinkler     [label="Sprinkler\n= True", fillcolor="#e5d5e7"];
//    WetGrass      [label="WetGrass",      fillcolor="#c6dbef"];
//    GreenerGrass  [label="GreenerGrass",  fillcolor="#c6dbef"];
//
//    // Edges
//    Cloudy -> Rain;
//    Sprinkler -> WetGrass;
//    Rain -> WetGrass;
//    WetGrass -> GreenerGrass;
//}
//```
//
//::::
//:::
//- $\Pr(S|C) = 1$ and $\Pr(W|R,S) = \Pr(W|R,S=T)$ and the joint probability
//  becomes:
//  \begingroup \small
//  $$
//  \Pr(C, R, W, G | do(S=True)) = \Pr(C) \Pr(R|C) \Pr(W|R,S=True) \Pr(G|W)
//  $$
//  \endgroup
//
//- Only descendants of manipulated variable $Sprinkler$ are affected
//
//* Intervention vs. Observation in Causal Models
//- **Intervention** conceptually _breaks_ normal causal dependencies
//  - Intervening on $Sprinkler$ removes causal link from $Weather$ to $Sprinkler$
//  - After intervention, causal graph excludes arrow $Weather \to Sprinkler$
//  - $Weather$ and $Sprinkler$ become independent under intervention
//
//- **Observation vs. Intervention**
//  - **Observation**: seeing $Sprinkler = T$
//    - Expressed as $\Pr(\cdot | Sprinkler = T)$
//    - Reflects _passive observation_ — sprinkler on provides information about
//      weather
//    - Since $Weather$ influences $Sprinkler$, observing $Sprinkler = T$ makes it
//      _less likely_ $Weather$ is cloudy
//  - **Intervention**: forcing $Sprinkler = T$
//    - Expressed as $\Pr(\cdot | do(Sprinkler = T))$
//    - _Active manipulation_ — set sprinkler on regardless of weather
//    - Causal link from $Weather$ to $Sprinkler$ is cut, weather distribution
//      remains unchanged
//
//- **Key intuition**
//  - Observation $\rightarrow$ correlation (information flows along causal links)
//  - Intervention $\rightarrow$ causation (links into manipulated variable are
//    removed)
//  - Thus, $\Pr(Weather | Sprinkler = T) \neq \Pr(Weather | do(Sprinkler = T))$
//
//* Controlling for a Variable in Causal Analysis
//
//- **Definition**
//  - To _control_ a variable means to hold it constant (statistically or
//    experimentally) to isolate the causal effect of another variable
//
//- **Example**
//  - Does exercise ($X$) cause weight loss ($Y$)?
//  - Confounder: Diet ($Z$) affects both exercise and weight
//  - By controlling for diet (e.g., comparing people with similar diets), you can
//    estimate the effect of exercise more accurately
//
//- In **regression analysis**
//  - Include $Z$ as an additional independent variable
//  - E.g., in $Y = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon$
//    - $\beta_1$ measures the effect of $X$ *controlling for* $Z$
//    - Coefficient $\beta_1$ = _change in $Y$_ with a one-unit change in $X_1$,
//      _holding $X_2$ constant_
//    - Isolates $X_1$'s unique contribution
//    - Compares individuals with the same $X_2$ but different $X_1$
//
//- In **experiments**
//  - Keep $Z$ constant or randomize it

## Type of Variables in Causal AI

* Mediator Variable
::: columns
:::: {.column width=50%}
- A **mediator variable** $M$
  - Is an intermediate variable that _transmits_ the causal effect from $X$
    (treatment) to $Y$ (outcome)
  - Lies **on the causal path** between $X$ and $Y$
  - Captures the **mechanism or process** through which $X$ influences $Y$

::::
:::: {.column width=45%}
```graphviz
digraph CausalFlow {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=LR;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [fillcolor="#A6C8F4", label="Treatment\n(X)"];
    M [fillcolor="#F4A6A6", label="Mediator\n(M)"];
    Y [fillcolor="#A6C8F4", label="Outcome\n(Y)"];

    X -> M;
    M -> Y;
}
```
::::
:::

* Mediator Variable: Example

::: columns
:::: {.column width=65%}
- **Research question**: _"Does a training program increase employee
  productivity?"_

- Causal effect may be indirect, through a **mediator**
  - Training might not immediately boost productivity
  - Could enhance job satisfaction, raising productivity

- **Causal interpretation**
  - $X$: Training Program (cause)
  - $M$: Job Satisfaction (mediator)
  - $Y$: Employee Productivity (effect)
  - Path: $X \rightarrow M \rightarrow Y$

- **Direct vs. Indirect effects**
  - _Indirect effect_ $X$ affects $Y$ through $M$
  - _Direct effect_ $X$ affects $Y$ not through $M$
  - Controlling for $M$ separates effects, clarifying training impact

::::
:::: {.column width=35%}

```graphviz
digraph CausalFlow {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [label="Training program", fillcolor="#FFD1A6", color=black];
    M [label="Job satisfaction", fillcolor="#F4A6A6", color=black];
    Y [label="Employee productivity", fillcolor="#A6E7F4", color=black];

    X -> M;
    M -> Y;
}
```
::::
:::

* Moderator Variable
::: columns
:::: {.column width=50%}
- A **moderator variable** $M$
  - Changes the _strength_ or _direction_ of the relationship between an
    independent variable $(X)$ and a dependent variable $(Y)$
  - Is not part of the causal chain but conditions the relationship
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [label="Treatment\n(X)", fillcolor="#A6C8F4", pos="0,1!"];
    Y [label="Outcome\n(Y)", fillcolor="#A6C8F4", pos="2,1!"];
    M [label="Moderator (M)\nVariable", pos="1,0!", fillcolor="#F4A6A6"];
    XY [label="", shape=point, width=0.01, style=invis, pos="1,1!"];

    X -> XY [arrowhead=none];
    XY -> Y;
    M -> XY;

    // Use neato for fixed positioning
    layout=neato;
  }
```
::::
:::

* Moderator Variable: Example

::: columns
:::: {.column width=50%}
- **Research question**: _"Study relationship between stress $X$ and job
  performance $Y$"_

- **Social support** $M$ as a moderator
  - High social support weakens stress's negative effect on performance
  - Low social support strengthens stress's negative effect on performance
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [label="Stress", fillcolor="#FFD1A6", pos="0,1!"];
    Y [label="Job\nPerformance", fillcolor="#A6E7F4", pos="2,1!"];
    M [label="Social Support", pos="1,0!", fillcolor="#F4A6A6"];
    XY [label="", shape=circle, width=0.01, style=invis, pos="1,1!"];

    X -> XY [arrowhead=none];
    XY -> Y;
    M -> XY;

    // Use neato for fixed positioning
    layout=neato;
}
```
::::
:::

* Confounder Variable
::: columns
:::: {.column width=60%}

- A **confounder** $C$
  - Affects both treatment (cause) and outcome
  - Creates misleading association if not controlled

::::
:::: {.column width=35%}
```graphviz
digraph CausalTriangle {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Confounder [label="Confounder (C)", fillcolor="#F4A6A6"];
    Treatment  [label="Treatment (X)", fillcolor="#A6C8F4"];
    Outcome    [label="Outcome (Y)", fillcolor="#A6C8F4"];

    Confounder -> Treatment;
    Confounder -> Outcome;
    Treatment  -> Outcome;

    {rank=same; Treatment; Outcome;}
}
```
::::
:::

* Confounder Variable: Example

::: columns
:::: {.column width=50%}
- $IceCreamSales$ and $Drowning$ move together
  - Correlation-based model claims association, but how to use this
    relationship?
  - Ban ice cream to prevent drowning?
  - Ice cream maker increase drowning to boost sales?

::::
:::: {.column width=30%}
```tikz
\begin{axis}[
    xlabel={Ice cream sales},
    ylabel={Drownings},
    width=10cm,
    height=6cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    title={Observed association}
]
% rising trend due to common cause "Summer"
\addplot+[only marks, mark=*, mark size=1.8pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.7) (1.2,1.1) (1.6,1.6)
    (2.1,2.4) (2.8,3.0) (3.4,3.6) (4.2,4.7) (5.1,6.0) (6.0,7.8)
};
% visual cue of spurious linear fit
%\addplot[samples=2,domain=0.2:6.2, very thick, color=red!60, dashed] {1.25*x-0.9};
\end{axis}
```
::::
:::: {.column width=20%}

```graphviz
digraph CausalTriangle {
    rankdir=TB;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    IceCreamSales [label="IceCreamSales", fillcolor="#A6C8F4"];
    Drowning [label="Drowning", fillcolor="#F4A6A6"];

    {rank=same; IceCreamSales; Drowning;}

    IceCreamSales -> Drowning;
    Drowning -> IceCreamSales;
}
```
::::
:::

\vspace{0.5cm}

::: columns
:::: {.column width=50%}
- No cause-effect between $IceCreamSales$ and $Drowning$
  - $Temperature$ is a confounder
  - Control for season in regression or intervention, association disappears

::::
:::: {.column width=30%}

```tikz
\begin{axis}[
    xlabel={Ice cream sales},
    ylabel={Drownings},
    width=10cm,
    height=6cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    title={After controlling for temperature}
]
% group 1: low temperature days
\addplot+[only marks, mark=*, mark size=1.8pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.3) (1.2,0.2) (1.6,0.4)
};
% group 2: medium temperature days
\addplot+[only marks, mark=triangle*, mark size=2pt, color=orange!70] coordinates {
    (2.1,1.1) (2.8,1.2) (3.4,1.0) (3.9,1.1)
};
% group 3: hot days
\addplot+[only marks, mark=square*, mark size=2pt, color=red!70] coordinates {
    (4.2,2.1) (5.1,2.0) (6.0,2.2) (6.5,2.1)
};
\node[anchor=south east, font=\small, text=black!70] at (rel axis cs:1.0,0.05)
{No trend within each temperature band};
\end{axis}
```

```tikz
\begin{axis}[
    xlabel={Ice Cream Sales},
    ylabel={Drownings},
    width=11cm,
    height=6.5cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=0.9pt,
    ymin=0,
    ymax=2.5,
    title={No relationship once temperature is controlled}
]
% Cool days (low temperature)
\addplot+[only marks, mark=*, mark size=2pt, color=blue!60] coordinates {
    (0.3,0.2) (0.8,0.3) (1.2,0.2) (1.6,0.4) (2.1,0.3) (2.8,0.2) (3.4,0.4) (3.9,0.5) (4.2,0.4) (5.1,0.3) (6.0,0.2) (6.5,0.2)
};
\node[anchor=south east, font=\small, text=black!70] at (rel axis cs:0.95,0.08)
{Flat: no causal effect};
\end{axis}
```

::::
:::: {.column width=20%}
```graphviz
digraph CausalTriangle {
    rankdir=TB;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Temperature [label="Temperature", fillcolor="#FFD1A6"];
    IceCreamSales [label="IceCreamSales", fillcolor="#A6C8F4"];
    Drowning [label="Drowning", fillcolor="#F4A6A6"];

    {rank=same; IceCreamSales; Drowning;}

    Temperature -> IceCreamSales;
    Temperature -> Drowning;
}
```

::::
:::

* Collider
::: columns
:::: {.column width=60%}
- A **collider** $A$
  - Is a variable influenced by multiple variables $B$, $C$
  - Complicates understanding relationships between variables $B, C$ and those it
    influences, $Y$
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  B [label="Cause 1 (B)", fillcolor="#A6C8F4"];
  C [label="Cause 2 (C)", fillcolor="#A6C8F4"];
  A [label="Collider (A)", fillcolor="#F4A6A6"];
  Y [label="Effect (Y)", fillcolor="#A6C8F4"];

  B -> A;
  C -> A;
  A -> Y;

  {rank=same; B; C;}
}
```
::::
:::

* Collider: Examples
::: columns
:::: {.column width=60%}
- Study the relationship between $Exercise$ and $Heart Disease$
  - $Diet$ and $Exercise$ influence $Body Weight$
  - $Body Weight$ influences $Heart Disease$
  - $Body Weight$ is a collider
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise (E)", fillcolor="#A6C8F4"]
  D [label="Diet (D)", fillcolor="#A6C8F4"]
  W [label="Body Weight (W)", fillcolor="#F4A6A6"];
  H [label="Heart Disease (H)", fillcolor="#A6C8F4"];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

* Collider Bias
- Aka "Berkson's paradox"

- **Conditioning on a collider** can introduce a spurious association between its
  parents by _"opening a path that is blocked"_

::: columns
:::: {.column width=70%}
- Consider the variables:
  - $Diet$ (D)
  - $Exercise$ (E)
  - $BodyWeight$ (W)
  - $HeartDisease$ (D)

- **Without conditioning on $W$**
  - $E$ and $D$ are independent
    - E.g., knowing exercise level $E$ doesn't inform about diet $D$, and vice
      versa
  - Collider $W$ blocks association between $E$ and $D$
::::
:::: {.column width=25%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise (E)", fillcolor="#A6C8F4"]
  D [label="Diet (D)", fillcolor="#A6C8F4"]
  W [label="Body Weight (W)", fillcolor="#F4A6A6"];
  H [label="Heart Disease (H)", fillcolor="#A6C8F4"];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

- **After conditioning on $W$**
  - E.g., individuals with specific body weight
  - Introduce dependency between $E$ and $D$
  - With $W$ fixed, changes in $E$ balanced by changes in $D$, inducing spurious
    correlation between $E$ and $D$

// TODO: Add an example in the tutorial

## Paths

* Fork Structure
::: columns
:::: {.column width=60%}
- A **fork** $D \leftarrow X \rightarrow C$ occurs when a single variable
  causally influences two or more variables
  - $X$ is a **confounder** (common cause) of $C$ and $D$
  - Forks induce statistical dependence between $C$ and $D$
    even if $C$ and $D$ are not causally linked

- **Conditioning** on $X$ blocks the path and removes spurious correlation
::::
:::: {.column width=35%}
```graphviz
digraph CausalDAG {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [fillcolor="#F4A6A6"];
    C [fillcolor="#A6C8F4"];
    D [fillcolor="#A6E7F4"];

    {rank=same; C; D;}

    X -> C;
    X -> D;
}
```
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=60%}
- **Example**
  - $Lifestyle$ is a confounder that affects both $Weight$ and $BloodPressure$
  - These outcomes may appear correlated due to shared cause
::::
:::: {.column width=35%}
```graphviz
digraph CausalDAG {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Lifestyle [label="Lifestyle", fillcolor="#FFD1A6"];
    Weight [label="Weight", fillcolor="#B2E2B2"];
    BP [label="Blood\nPressure", fillcolor="#A6E7F4"];

    {rank=same; Weight; BP;}

    Lifestyle -> Weight;
    Lifestyle -> BP;
}
```
::::
:::

* Inverted Fork

::: columns
:::: {.column width=60%}
- An **inverted fork** occurs when two or more arrows converge on a common node
  - **Colliders** block associations unless the collider or its descendants are
    conditioned on

- **Conditioning on a collider** _opens a path_, inducing spurious correlations
  - This is the basis of selection bias
::::
:::: {.column width=35%}
```graphviz
digraph ColliderExample {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [label="X", fillcolor="#F4A6A6"];
    Y [label="Y", fillcolor="#A6C8F4"];
    Z [label="Z", fillcolor="#A6E7F4"];

    {rank=same; X; Y;}

    X -> Z;
    Y -> Z;
}
```
::::
:::

\vspace{0.5cm}

::: columns
:::: {.column width=60%}
- **Example**
  - Sales influenced by multiple independent causes
  - $MarketingSpend$ and $ProductQuality$ both influence $Sales$
  - Conditioning on $Sales$ can induce false dependence between $MarketingSpend$
    and $ProductQuality$
::::
:::: {.column width=35%}
```graphviz
digraph ImprovedDiagram {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Marketing [label="Marketing Spend", fillcolor="#F4A6A6"];
    Quality [label="Product Quality", fillcolor="#B2E2B2"];
    Sales [label="Sales", fillcolor="#A6C8F4"];

    {rank=same; Marketing; Quality;}

    Marketing -> Sales;
    Quality -> Sales;
}
```
::::
:::

* Path connecting unobserved variables
- **Unobserved variables** affect the model but we don't have a direct measure of
  it

::: columns
:::: {.column width=60%}
- E.g., consider the causal DAG
  - A retailer does market research, expecting $Price$ to influence $Sales$ in a
    predictable way
  - A retailer sets the $Price$ of a new product based on market research
  - The retailer can observe and measure $Behavior$, e.g.,
    - Discounts
    - Promotional campaign
  - There are unobserved vars that influence the model, e.g.,
    - Social media buzz
    - Word-of-mouth recommendation
::::
:::: {.column width=35%}
```graphviz
digraph CausalDAG {
  layout=neato;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Node positions
  U [label="Unobserved\nVariables", pos="2,1!"];
  M [label="Price", pos="1,1!"];
  X [label="Behavior", pos="0,0!"];
  Y [label="Sales", pos="2,0!"];

  M -> X;
  M -> Y;
  X -> Y;
  U -> Y [style=dashed];

  // Prevent layout engine from moving nodes
  edge [splines=true];
}
```
::::
:::

* Front-door Paths in Causal Inference
- A front-door path reveals causal influence through an observable mediator
  - The causal effect flows: $A \rightarrow P \rightarrow S$
- Requirements for identifiability:
  - All confounders of $A \rightarrow P$ and $P \rightarrow S$ are observed and
    controlled
  - There are no back-door paths from $A$ to $S$ through unobserved variables
- Enables causal estimation when back-door adjustment is infeasible
- Example:
  - Advertising impacts sales through customer perception of price
  - $A$: Advertising, $P$: Price perception, $S$: Sales
- Pearl's front-door criterion provides a formal method for adjustment
  - Estimate $P(P|A)$, $P(S|P,A)$, and $P(A)$ from data to compute causal effect

```graphviz[width=50%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style=rounded];

  A [label="Advertising"];
  P [label="Price"];
  S [label="Sales"];

  A -> P;
  P -> S;
  A -> S [style=dotted, label="confounding path"];

  {rank=same; P; S;}
}
```

// RESUME

* Back-Door Paths
- A company wants to understand the causal effect of price on sales

  ```graphviz[width=50%]
  digraph CausalDAG {
    rankdir=TB;
    node [shape=box, style=rounded];

    Price [label="Price"];
    Sales [label="Sales"];
    AdvSpend [label="Advertisement Spend"];

    Price -> Sales;
    AdvSpend -> Price;
    AdvSpend -> Sales;

    {rank=same; Price; Sales;}
  }
  ```

- `Price` $\rightarrow$ `Sales` is the front-door path

- A confounder is `Advertising spend` since it can affect both:
  - The price the company can set (e.g., the cost increases to cover
    advertisement costs and the product is perceived as more valuable)
  - The sales (directly)

- The back-door path goes from `Sales` to `Price` via `Advertising spend`

- The company needs to control for `Advertising spend` to estimate the causal
  effect of `Price` on `Sales` by:
  - Using `Advertising spend` as covariate in the regression
  - Designing experiment holding `Advertising spend` constant or randomized

* Frontdoor and Backdoor Paths

- Question: _Will increasing our customer satisfaction increase our sales?_

::: columns
:::: {.column width=45%}
- Assume that the Causal DAG is
  ```graphviz
  digraph CausalGraph {
    rankdir=TB;
    bgcolor="transparent";

    node [shape=box, style="rounded,filled", fontname="Helvetica"];

    // Define nodes
    PQ [label="Product Quality", fillcolor="#ffcccc", color="#ff0000", penwidth=2];
    CS [label="Cust Satisfaction", fillcolor="white"];
    S [label="Sales", fillcolor="white"];

    // Force PQ on top by putting CS and S in the same rank
    { rank = same; CS; S }

    // Define edges
    PQ -> CS;
    PQ -> S;
    CS -> S;
  }
  ```
- **Front-door path** (i.e., a direct causal relationship):
  $CustomerSatisfaction \rightarrow Sales$
::::
:::: {.column width=45%}
- **Backdoor path**: $ProductQuality$ is a common cause (confounder) of
  both $CustomerSatisfaction$ and $Sales$
- To analyze the relationship between customer satisfaction and sales, we need
  to:
  - Control for $ProductQuality$ to close the backdoor path
  - Eliminate the confounding effect
- In reality there are more confounding effects (e.g., price)
::::
:::


* Counterfactuals

- A **counterfactual** describes what would have happened under a different
  scenario
  - _"What would the outcome have been _if_ X had been different?"_
  - _"If kangaroos had no tails, they would topple over"_
  - _"What if we had two suppliers of our product, rather than one? Would we
    have more sales?"_
  - _"Would customers be more satisfied if we could ship products in one week,
    rather than three weeks?"_

- **Causal reasoning**:
  - Goes beyond correlation and association
  - Requires a causal model (like an SCM) to simulate alternate realities
  - E.g.,
    - Actual: A student received tutoring and scored 85%
    - Counterfactual: What if the student didn't receive tutoring?
    - Causal model estimates the alternative outcome (e.g., 70%)

- **Challenges**:
  - Requires strong assumptions and accurate models
  - Difficult to validate directly since counterfactuals are unobservable
